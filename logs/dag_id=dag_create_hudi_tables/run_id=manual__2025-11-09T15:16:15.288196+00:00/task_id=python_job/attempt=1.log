[2025-11-09T15:16:17.107+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_create_hudi_tables.python_job manual__2025-11-09T15:16:15.288196+00:00 [queued]>
[2025-11-09T15:16:17.116+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_create_hudi_tables.python_job manual__2025-11-09T15:16:15.288196+00:00 [queued]>
[2025-11-09T15:16:17.116+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-11-09T15:16:17.129+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-11-09 15:16:15.288196+00:00
[2025-11-09T15:16:17.135+0000] {standard_task_runner.py:57} INFO - Started process 1189 to run task
[2025-11-09T15:16:17.138+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_create_hudi_tables', 'python_job', 'manual__2025-11-09T15:16:15.288196+00:00', '--job-id', '17', '--raw', '--subdir', 'DAGS_FOLDER/my_dag.py', '--cfg-path', '/tmp/tmpp0gqin10']
[2025-11-09T15:16:17.140+0000] {standard_task_runner.py:85} INFO - Job 17: Subtask python_job
[2025-11-09T15:16:17.192+0000] {task_command.py:415} INFO - Running <TaskInstance: dag_create_hudi_tables.python_job manual__2025-11-09T15:16:15.288196+00:00 [running]> on host 863c17993851
[2025-11-09T15:16:17.284+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Soumil Shah' AIRFLOW_CTX_DAG_ID='dag_create_hudi_tables' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-11-09T15:16:15.288196+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-11-09T15:16:15.288196+00:00'
[2025-11-09T15:16:17.298+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-11-09T15:16:17.299+0000] {spark_submit.py:403} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.executor.instances=1 --packages org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0,org.apache.hadoop:hadoop-aws:3.3.2 --name arrow-spark --deploy-mode client jobs/my_dag_job.py
[2025-11-09T15:16:17.428+0000] {spark_submit.py:579} INFO - /home/***/.local/lib/python3.9/site-packages/pyspark/bin/load-spark-env.sh: line 68: ps: command not found
[2025-11-09T15:16:18.851+0000] {spark_submit.py:579} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-11-09T15:16:18.990+0000] {spark_submit.py:579} INFO - Ivy Default Cache set to: /home/***/.ivy2/cache
[2025-11-09T15:16:18.991+0000] {spark_submit.py:579} INFO - The jars for the packages stored in: /home/***/.ivy2/jars
[2025-11-09T15:16:19.004+0000] {spark_submit.py:579} INFO - org.apache.hudi#hudi-spark3.4-bundle_2.12 added as a dependency
[2025-11-09T15:16:19.006+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-11-09T15:16:19.008+0000] {spark_submit.py:579} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-40a046bd-bd9f-4817-9f22-ff162a2b9703;1.0
[2025-11-09T15:16:19.009+0000] {spark_submit.py:579} INFO - confs: [default]
[2025-11-09T15:16:19.192+0000] {spark_submit.py:579} INFO - found org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0 in central
[2025-11-09T15:16:19.226+0000] {spark_submit.py:579} INFO - found org.apache.hadoop#hadoop-aws;3.3.2 in central
[2025-11-09T15:16:19.254+0000] {spark_submit.py:579} INFO - found com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central
[2025-11-09T15:16:19.277+0000] {spark_submit.py:579} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-11-09T15:16:19.308+0000] {spark_submit.py:579} INFO - :: resolution report :: resolve 285ms :: artifacts dl 14ms
[2025-11-09T15:16:19.308+0000] {spark_submit.py:579} INFO - :: modules in use:
[2025-11-09T15:16:19.308+0000] {spark_submit.py:579} INFO - com.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]
[2025-11-09T15:16:19.308+0000] {spark_submit.py:579} INFO - org.apache.hadoop#hadoop-aws;3.3.2 from central in [default]
[2025-11-09T15:16:19.308+0000] {spark_submit.py:579} INFO - org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0 from central in [default]
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - |                  |            modules            ||   artifacts   |
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
[2025-11-09T15:16:19.309+0000] {spark_submit.py:579} INFO - ---------------------------------------------------------------------
[2025-11-09T15:16:19.313+0000] {spark_submit.py:579} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-40a046bd-bd9f-4817-9f22-ff162a2b9703
[2025-11-09T15:16:19.314+0000] {spark_submit.py:579} INFO - confs: [default]
[2025-11-09T15:16:19.319+0000] {spark_submit.py:579} INFO - 0 artifacts copied, 4 already retrieved (0kB/6ms)
[2025-11-09T15:16:19.502+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-11-09T15:16:20.934+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO SparkContext: Running Spark version 3.4.0
[2025-11-09T15:16:20.954+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO ResourceUtils: ==============================================================
[2025-11-09T15:16:20.955+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-11-09T15:16:20.955+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO ResourceUtils: ==============================================================
[2025-11-09T15:16:20.955+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO SparkContext: Submitted application: arrow-spark
[2025-11-09T15:16:20.974+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-11-09T15:16:20.981+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO ResourceProfile: Limiting resource is cpu
[2025-11-09T15:16:20.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:20 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-11-09T15:16:21.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SecurityManager: Changing view acls to: ***
[2025-11-09T15:16:21.042+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SecurityManager: Changing modify acls to: ***
[2025-11-09T15:16:21.042+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SecurityManager: Changing view acls groups to:
[2025-11-09T15:16:21.042+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SecurityManager: Changing modify acls groups to:
[2025-11-09T15:16:21.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: ***; groups with view permissions: EMPTY; users with modify permissions: ***; groups with modify permissions: EMPTY
[2025-11-09T15:16:21.270+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO Utils: Successfully started service 'sparkDriver' on port 37143.
[2025-11-09T15:16:21.309+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkEnv: Registering MapOutputTracker
[2025-11-09T15:16:21.332+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkEnv: Registering BlockManagerMaster
[2025-11-09T15:16:21.350+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-11-09T15:16:21.350+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-11-09T15:16:21.354+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-11-09T15:16:21.370+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-93b692bb-b4a8-4a34-891f-14c166b7ccc0
[2025-11-09T15:16:21.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-11-09T15:16:21.397+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-11-09T15:16:21.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-11-09T15:16:21.576+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-11-09T15:16:21.604+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar at spark://863c17993851:37143/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar with timestamp 1762701380927
[2025-11-09T15:16:21.605+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar at spark://863c17993851:37143/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar with timestamp 1762701380927
[2025-11-09T15:16:21.605+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar at spark://863c17993851:37143/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar with timestamp 1762701380927
[2025-11-09T15:16:21.605+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added JAR file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://863c17993851:37143/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1762701380927
[2025-11-09T15:16:21.607+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar at spark://863c17993851:37143/files/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar with timestamp 1762701380927
[2025-11-09T15:16:21.608+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar to /tmp/spark-f5a6d9e3-cfca-41d1-86fe-bc0566552d5b/userFiles-add38435-0db2-40de-aae6-3a9caf6201b1/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar
[2025-11-09T15:16:21.805+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar at spark://863c17993851:37143/files/org.apache.hadoop_hadoop-aws-3.3.2.jar with timestamp 1762701380927
[2025-11-09T15:16:21.805+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO Utils: Copying /home/***/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar to /tmp/spark-f5a6d9e3-cfca-41d1-86fe-bc0566552d5b/userFiles-add38435-0db2-40de-aae6-3a9caf6201b1/org.apache.hadoop_hadoop-aws-3.3.2.jar
[2025-11-09T15:16:21.811+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO SparkContext: Added file file:///home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar at spark://863c17993851:37143/files/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar with timestamp 1762701380927
[2025-11-09T15:16:21.812+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:21 INFO Utils: Copying /home/***/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar to /tmp/spark-f5a6d9e3-cfca-41d1-86fe-bc0566552d5b/userFiles-add38435-0db2-40de-aae6-3a9caf6201b1/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar
[2025-11-09T15:16:22.167+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO SparkContext: Added file file:///home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://863c17993851:37143/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1762701380927
[2025-11-09T15:16:22.167+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO Utils: Copying /home/***/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-f5a6d9e3-cfca-41d1-86fe-bc0566552d5b/userFiles-add38435-0db2-40de-aae6-3a9caf6201b1/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-11-09T15:16:22.235+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-11-09T15:16:22.292+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.5:7077 after 36 ms (0 ms spent in bootstraps)
[2025-11-09T15:16:22.364+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251109151622-0001
[2025-11-09T15:16:22.366+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251109151622-0001/0 on worker-20251109144903-172.19.0.9-38701 (172.19.0.9:38701) with 4 core(s)
[2025-11-09T15:16:22.369+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20251109151622-0001/0 on hostPort 172.19.0.9:38701 with 4 core(s), 1024.0 MiB RAM
[2025-11-09T15:16:22.374+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44651.
[2025-11-09T15:16:22.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO NettyBlockTransferService: Server created on 863c17993851:44651
[2025-11-09T15:16:22.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-11-09T15:16:22.382+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 863c17993851, 44651, None)
[2025-11-09T15:16:22.386+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO BlockManagerMasterEndpoint: Registering block manager 863c17993851:44651 with 434.4 MiB RAM, BlockManagerId(driver, 863c17993851, 44651, None)
[2025-11-09T15:16:22.389+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 863c17993851, 44651, None)
[2025-11-09T15:16:22.391+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 863c17993851, 44651, None)
[2025-11-09T15:16:22.428+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251109151622-0001/0 is now RUNNING
[2025-11-09T15:16:22.616+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-11-09T15:16:23.313+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:23 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-11-09T15:16:23.315+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:23 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-11-09T15:16:25.002+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:25 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.9:56384) with ID 0,  ResourceProfileId 0
[2025-11-09T15:16:25.075+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:25 INFO BlockManagerMasterEndpoint: Registering block manager 172.19.0.9:35743 with 434.4 MiB RAM, BlockManagerId(0, 172.19.0.9, 35743, None)
[2025-11-09T15:16:26.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO CodeGenerator: Code generated in 177.861946 ms
[2025-11-09T15:16:26.971+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-11-09T15:16:26.987+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-09T15:16:26.988+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2025-11-09T15:16:26.990+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:26.992+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:27.000+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:26 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-09T15:16:27.108+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:27 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.6 KiB, free 434.4 MiB)
[2025-11-09T15:16:27.340+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:27 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 434.4 MiB)
[2025-11-09T15:16:27.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:27 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 863c17993851:44651 (size: 6.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:27.348+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:27 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:27.367+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:27 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:27.369+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:27 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-11-09T15:16:29.581+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:29 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 11853 bytes)
[2025-11-09T15:16:30.026+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:30 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on 172.19.0.9:35743 (size: 6.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:31.265+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1691 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:31.268+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-11-09T15:16:31.278+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 58815
[2025-11-09T15:16:31.284+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 4.264 s
[2025-11-09T15:16:31.288+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:31.289+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-11-09T15:16:31.292+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 4.322887 s
[2025-11-09T15:16:31.343+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO CodeGenerator: Code generated in 21.766226 ms
[2025-11-09T15:16:31.358+0000] {spark_submit.py:579} INFO - +--------------------+----------------+--------------+-------------------+--------------------+--------------------+--------------------+
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |         customer_id|            name|         state|               city|               email|          created_at|             address|
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - +--------------------+----------------+--------------+-------------------+--------------------+--------------------+--------------------+
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |75a81bed-f6c6-422...|     James Smith|    Washington|           Seanview| scott70@example.com|2025-11-09T15:16:...|6019 Jennings Gle...|
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |ee2d05f1-4ddf-41e...|  Jennifer Cross|North Carolina|        Anthonyside|davidwilson@examp...|2025-11-09T15:16:...|760 Christina For...|
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |fc92eed4-8951-473...| Jessica Barrett|      Kentucky|    Port Alisonbury|teresa29@example.com|2025-11-09T15:16:...|19870 Rogers Park...|
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |d8c53bea-dc0e-451...|       Corey Lee|       Florida|        Lake Andrea|benjaminmartinez@...|2025-11-09T15:16:...|993 Austin Hills\...|
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |5242e93d-3c6e-4be...|    Casey Gentry|North Carolina|         Stevenbury|smithmackenzie@ex...|2025-11-09T15:16:...|4925 Perez Mounta...|
[2025-11-09T15:16:31.359+0000] {spark_submit.py:579} INFO - |c762429a-255a-443...|Amanda Rodriguez|       Georgia|        Pamelamouth|catherinegray@exa...|2025-11-09T15:16:...|651 Mary Inlet Su...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |5030d017-cb4c-400...|   Robert Morrow|     Minnesota|    North Christian| mdodson@example.net|2025-11-09T15:16:...|USNS Burke\nFPO A...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |63859c8d-af8c-44f...|    Alex Vazquez|      Colorado|       Brownchester|dorseysharon@exam...|2025-11-09T15:16:...|304 Justin Inlet\...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |42278693-7354-427...|     Jeremy Cobb|     Tennessee|          New Scott| gdorsey@example.net|2025-11-09T15:16:...|Unit 6771 Box 869...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |b8627e2c-1fd1-489...|  Cheryl Andrade|        Kansas| Lake Cherylborough|zjimenez@example.net|2025-11-09T15:16:...|3465 Ferrell Miss...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |6a5d0277-5e9c-41b...|  Brenda Proctor|      Arkansas|         New Nicole|marymontoya@examp...|2025-11-09T15:16:...|125 Mary Fields S...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |9ef3d659-53e3-42e...|      Dawn Leach|        Hawaii|          West Evan|ngoodman@example.com|2025-11-09T15:16:...|281 Castro Knoll\...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |ed748d9a-ad35-4aa...|Jessica Cardenas|      Virginia|       Petersonland|schneiderjuan@exa...|2025-11-09T15:16:...|02681 Julia Grove...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |a9ee0b62-591f-429...|   Amanda Wilson|      Virginia|       Jennifertown| jbailey@example.com|2025-11-09T15:16:...|1993 Crosby Villa...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |e2ce36d1-5bfe-4d0...|   Brian Johnson|      Kentucky|     West Juliaside|dsullivan@example...|2025-11-09T15:16:...|658 Donna Center ...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |7f784ece-8d63-421...|      Holly Soto|       Montana|   West Claytonport| allen53@example.net|2025-11-09T15:16:...|9215 Cynthia Vill...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |c9c98d3f-9eda-43b...|    Susan Hanson|   Mississippi|  Lake Jenniferport|thomas09@example.org|2025-11-09T15:16:...|8201 Wallace Rue\...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |327310d2-d4de-404...|  Gerald Padilla| West Virginia|Lake Vanessachester|  jack48@example.net|2025-11-09T15:16:...|83326 Nguyen Alle...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |ba8cc312-4006-4d2...|    Mason Moreno|  South Dakota|         Autumnfort|cooperstephanie@e...|2025-11-09T15:16:...|USNS Miller\nFPO ...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - |44ee3a8e-1683-4b6...|  Michael Cooper|      Michigan|  Port Heathermouth| donna51@example.net|2025-11-09T15:16:...|250 Andrew Island...|
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - +--------------------+----------------+--------------+-------------------+--------------------+--------------------+--------------------+
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - only showing top 20 rows
[2025-11-09T15:16:31.360+0000] {spark_submit.py:579} INFO - 
[2025-11-09T15:16:31.455+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO CodeGenerator: Code generated in 13.4901 ms
[2025-11-09T15:16:31.473+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-11-09T15:16:31.474+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-09T15:16:31.474+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2025-11-09T15:16:31.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:31.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:31.476+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-09T15:16:31.486+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.3 KiB, free 434.4 MiB)
[2025-11-09T15:16:31.498+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.4 MiB)
[2025-11-09T15:16:31.500+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 863c17993851:44651 (size: 6.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:31.502+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:31.504+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:31.505+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-11-09T15:16:31.508+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 10196 bytes)
[2025-11-09T15:16:31.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 863c17993851:44651 in memory (size: 6.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:31.515+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO BlockManagerInfo: Removed broadcast_0_piece0 on 172.19.0.9:35743 in memory (size: 6.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:31.537+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on 172.19.0.9:35743 (size: 6.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:31.613+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 105 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:31.613+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-11-09T15:16:31.614+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.137 s
[2025-11-09T15:16:31.615+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:31.615+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-11-09T15:16:31.616+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.142899 s
[2025-11-09T15:16:31.638+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO CodeGenerator: Code generated in 14.554537 ms
[2025-11-09T15:16:31.640+0000] {spark_submit.py:579} INFO - +--------------------+--------------------+-----------+--------+----------+--------------------+
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |            order_id|                name|order_value|priority|order_date|         customer_id|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - +--------------------+--------------------+-----------+--------+----------+--------------------+
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |59ae0de5-4f6f-418...|Recent there middle.|        158|     LOW|2025-11-07|4e0af661-3dd2-41f...|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |ceb75287-93db-4d0...|     Arm understand.|        801|     LOW|2025-10-27|9529d158-7f0a-4e1...|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |9b429fba-3cce-4c1...| Democratic outside.|        221|  MEDIUM|2025-10-25|7ad2c1a6-316c-4d1...|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |10abde08-3c21-4cf...|   Five however get.|        672|  MEDIUM|2025-10-17|a9ee0b62-591f-429...|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |99976c1c-3221-4aa...|       Small senior.|        977|    HIGH|2025-10-26|c762429a-255a-443...|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |206c5c25-9d2e-405...|          Affect PM.|        610|    HIGH|2025-10-10|607a3f38-cff6-418...|
[2025-11-09T15:16:31.641+0000] {spark_submit.py:579} INFO - |48960c76-cb12-436...| Positive she power.|        736|    HIGH|2025-11-03|af726385-ab64-405...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |9e72363e-36ab-4ab...|Customer onto wrong.|        993|     LOW|2025-10-21|e2ce36d1-5bfe-4d0...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |e75fa58c-6a60-47b...|Body shoulder court.|        567|     LOW|2025-10-20|5242e93d-3c6e-4be...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |760b08e0-e4ee-4af...|     Recognize best.|         34|    HIGH|2025-10-27|44ee3a8e-1683-4b6...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |9e9b500d-070b-482...|Quite he everything.|        529|     LOW|2025-10-16|6a5d0277-5e9c-41b...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |66d1767b-2b2d-410...|    Drive religious.|        618|     LOW|2025-10-16|faf33083-56f9-4f9...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |deb7ff77-8d6d-43a...|       Risk section.|        186|     LOW|2025-10-24|5030d017-cb4c-400...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |02134df5-b85b-4e1...|  Boy save sort eye.|        783|  MEDIUM|2025-10-15|f1254db7-fe50-43c...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |470bfbdd-910b-44f...|   Important anyone.|        414|     LOW|2025-11-03|42278693-7354-427...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |8e834e3b-a688-405...|   Night crime city.|        883|  MEDIUM|2025-10-19|af726385-ab64-405...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |59402623-d02d-479...|  This hour medical.|        886|     LOW|2025-10-23|c3bb5f19-e68a-428...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |76ef2d2f-b94e-4cf...| Professor discover.|         45|  MEDIUM|2025-10-11|75a81bed-f6c6-422...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |98bd0849-0038-4e1...|  Talk anything bed.|        529|  MEDIUM|2025-10-19|327310d2-d4de-404...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - |0bca5c2b-6f3d-44d...|Somebody sport word.|        938|     LOW|2025-10-25|3c0024ca-9daa-403...|
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - +--------------------+--------------------+-----------+--------+----------+--------------------+
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - only showing top 20 rows
[2025-11-09T15:16:31.642+0000] {spark_submit.py:579} INFO - 
[2025-11-09T15:16:31.820+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO CodeGenerator: Code generated in 9.819602 ms
[2025-11-09T15:16:31.848+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Registering RDD 15 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-11-09T15:16:31.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2025-11-09T15:16:31.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
[2025-11-09T15:16:31.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:31.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:31.854+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-09T15:16:31.871+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.9 KiB, free 434.4 MiB)
[2025-11-09T15:16:31.886+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.4 MiB)
[2025-11-09T15:16:31.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 863c17993851:44651 (size: 8.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:31.888+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:31.890+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:31.891+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2025-11-09T15:16:31.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 11842 bytes)
[2025-11-09T15:16:31.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 11856 bytes)
[2025-11-09T15:16:31.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:31 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on 172.19.0.9:35743 (size: 8.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.077+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 184 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:32.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 221 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:32.114+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-11-09T15:16:32.115+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.258 s
[2025-11-09T15:16:32.117+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:32.118+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:32.119+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: waiting: Set()
[2025-11-09T15:16:32.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:32.169+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO CodeGenerator: Code generated in 14.449907 ms
[2025-11-09T15:16:32.204+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
[2025-11-09T15:16:32.206+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-09T15:16:32.207+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)
[2025-11-09T15:16:32.207+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-11-09T15:16:32.207+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:32.208+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-09T15:16:32.216+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:32.237+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.3 MiB)
[2025-11-09T15:16:32.238+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 863c17993851:44651 (size: 5.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.238+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:32.239+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:32.239+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-11-09T15:16:32.244+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2025-11-09T15:16:32.271+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on 172.19.0.9:35743 (size: 5.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.292+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.0.9:56384
[2025-11-09T15:16:32.401+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 158 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:32.401+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-11-09T15:16:32.402+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.189 s
[2025-11-09T15:16:32.403+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:32.403+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-11-09T15:16:32.403+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.198536 s
[2025-11-09T15:16:32.659+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-11-09T15:16:32.670+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-11-09T15:16:32.671+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-11-09T15:16:32.890+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 863c17993851:44651 in memory (size: 5.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Removed broadcast_3_piece0 on 172.19.0.9:35743 in memory (size: 5.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.906+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 863c17993851:44651 in memory (size: 8.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Removed broadcast_2_piece0 on 172.19.0.9:35743 in memory (size: 8.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.928+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 863c17993851:44651 in memory (size: 6.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:32.930+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:32 INFO BlockManagerInfo: Removed broadcast_1_piece0 on 172.19.0.9:35743 in memory (size: 6.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:33.517+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:33 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
[2025-11-09T15:16:33.528+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:33 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
[2025-11-09T15:16:33.569+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:33 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=customers/ as hoodie table s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.159+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.175+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:34.219+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.220+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.275+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:34.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO EmbeddedTimelineService: Starting Timeline service !!
[2025-11-09T15:16:34.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO EmbeddedTimelineService: Overriding hostIp to (863c17993851) found in spark-conf. It was null
[2025-11-09T15:16:34.483+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:34.483+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:34.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO log: Logging initialized @16881ms to org.apache.hudi.org.apache.jetty.util.log.Slf4jLog
[2025-11-09T15:16:34.639+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Javalin:
[2025-11-09T15:16:34.640+0000] {spark_submit.py:579} INFO - __                      __ _            __ __
[2025-11-09T15:16:34.640+0000] {spark_submit.py:579} INFO - / /____ _ _   __ ____ _ / /(_)____      / // /
[2025-11-09T15:16:34.641+0000] {spark_submit.py:579} INFO - __  / // __ `/| | / // __ `// // // __ \    / // /_
[2025-11-09T15:16:34.641+0000] {spark_submit.py:579} INFO - / /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
[2025-11-09T15:16:34.641+0000] {spark_submit.py:579} INFO - \____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/
[2025-11-09T15:16:34.641+0000] {spark_submit.py:579} INFO - 
[2025-11-09T15:16:34.641+0000] {spark_submit.py:579} INFO - https://javalin.io/documentation
[2025-11-09T15:16:34.641+0000] {spark_submit.py:579} INFO - 
[2025-11-09T15:16:34.642+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Javalin: Starting Javalin ...
[2025-11-09T15:16:34.663+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1112 days old. Consider checking for a newer version.).
[2025-11-09T15:16:34.732+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.29+6-post-Debian-1deb11u1
[2025-11-09T15:16:34.819+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Server: Started @17205ms
[2025-11-09T15:16:34.820+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Javalin: Listening on http://localhost:34171/
[2025-11-09T15:16:34.820+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO Javalin: Javalin started in 180ms \o/
[2025-11-09T15:16:34.821+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO TimelineService: Starting Timeline server on port :34171
[2025-11-09T15:16:34.821+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO EmbeddedTimelineService: Started embedded timeline server at 863c17993851:34171
[2025-11-09T15:16:34.833+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
[2025-11-09T15:16:34.834+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-11-09T15:16:34.835+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-11-09T15:16:34.880+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.890+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:34.903+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.903+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.912+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:34.918+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO CleanerUtils: Cleaned failed attempts if any
[2025-11-09T15:16:34.922+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.931+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:34.941+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.941+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.949+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:34.956+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.966+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:34.978+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:34.991+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:35.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:35.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:35.008+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO BaseHoodieWriteClient: Generate a new instant time: 20251109151634225 action: commit
[2025-11-09T15:16:35.008+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieActiveTimeline: Creating a new instant [==>20251109151634225__commit__REQUESTED]
[2025-11-09T15:16:35.040+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:35.049+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:35.061+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:35.062+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:35.071+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:35.082+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:35.092+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:35.101+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:35.144+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
[2025-11-09T15:16:35.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:35.158+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=customers//.hoodie/metadata as hoodie table s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:35.449+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:35.462+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:35.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:35.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:35.565+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-11-09T15:16:35.567+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO DAGScheduler: Got job 4 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-11-09T15:16:35.567+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO DAGScheduler: Final stage: ResultStage 5 (collect at HoodieSparkEngineContext.java:116)
[2025-11-09T15:16:35.568+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:35.568+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:35.569+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-11-09T15:16:35.580+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:35.590+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:35.592+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 863c17993851:44651 (size: 36.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:35.594+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:35.595+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:35.595+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-11-09T15:16:35.604+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7371 bytes)
[2025-11-09T15:16:35.658+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:35 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on 172.19.0.9:35743 (size: 36.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:36.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1265 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:36.863+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-11-09T15:16:36.863+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: ResultStage 5 (collect at HoodieSparkEngineContext.java:116) finished in 1.293 s
[2025-11-09T15:16:36.864+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:36.864+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-11-09T15:16:36.864+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Job 4 finished: collect at HoodieSparkEngineContext.java:116, took 1.299066 s
[2025-11-09T15:16:36.881+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:36.881+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
[2025-11-09T15:16:36.882+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
[2025-11-09T15:16:36.895+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
[2025-11-09T15:16:36.896+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Got job 5 (count at HoodieJavaRDD.java:115) with 1 output partitions
[2025-11-09T15:16:36.896+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Final stage: ResultStage 6 (count at HoodieJavaRDD.java:115)
[2025-11-09T15:16:36.896+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:36.896+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:36.897+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Submitting ResultStage 6 (ParallelCollectionRDD[26] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
[2025-11-09T15:16:36.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.0 KiB, free 434.3 MiB)
[2025-11-09T15:16:36.905+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.3 MiB)
[2025-11-09T15:16:36.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 863c17993851:44651 (size: 1866.0 B, free: 434.4 MiB)
[2025-11-09T15:16:36.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:36.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ParallelCollectionRDD[26] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:36.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-11-09T15:16:36.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:36 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7405 bytes)
[2025-11-09T15:16:37.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on 172.19.0.9:35743 (size: 1866.0 B, free: 434.4 MiB)
[2025-11-09T15:16:37.056+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 147 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:37.057+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-11-09T15:16:37.059+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: ResultStage 6 (count at HoodieJavaRDD.java:115) finished in 0.159 s
[2025-11-09T15:16:37.059+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:37.060+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-11-09T15:16:37.060+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Job 5 finished: count at HoodieJavaRDD.java:115, took 0.164963 s
[2025-11-09T15:16:37.060+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
[2025-11-09T15:16:37.077+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
[2025-11-09T15:16:37.159+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
[2025-11-09T15:16:37.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Got job 6 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
[2025-11-09T15:16:37.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Final stage: ResultStage 7 (foreach at HoodieSparkEngineContext.java:155)
[2025-11-09T15:16:37.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:37.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:37.164+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Submitting ResultStage 7 (ParallelCollectionRDD[27] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
[2025-11-09T15:16:37.196+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 383.8 KiB, free 433.9 MiB)
[2025-11-09T15:16:37.210+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 126.6 KiB, free 433.8 MiB)
[2025-11-09T15:16:37.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 863c17993851:44651 (size: 126.6 KiB, free: 434.2 MiB)
[2025-11-09T15:16:37.212+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:37.213+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (ParallelCollectionRDD[27] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:37.213+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-11-09T15:16:37.214+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:37.237+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on 172.19.0.9:35743 (size: 126.6 KiB, free: 434.2 MiB)
[2025-11-09T15:16:37.593+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 379 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:37.594+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-11-09T15:16:37.595+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: ResultStage 7 (foreach at HoodieSparkEngineContext.java:155) finished in 0.431 s
[2025-11-09T15:16:37.596+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:37.596+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-11-09T15:16:37.596+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Job 6 finished: foreach at HoodieSparkEngineContext.java:155, took 0.436551 s
[2025-11-09T15:16:37.604+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:37.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:37.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-11-09T15:16:37.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:37.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:37.620+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-11-09T15:16:37.646+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:37.647+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:37.648+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.657+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.667+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.668+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.675+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:37.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.695+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:37.695+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:37.697+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
[2025-11-09T15:16:37.697+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.707+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.715+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.715+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:37.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO CleanerUtils: Cleaned failed attempts if any
[2025-11-09T15:16:37.722+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.730+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.738+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.738+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.744+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:37.752+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.758+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:37.759+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:37.759+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
[2025-11-09T15:16:37.759+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
[2025-11-09T15:16:37.781+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.789+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.796+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.797+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:37.804+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20251109151637771]}
[2025-11-09T15:16:37.815+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:37.825+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:37.825+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:37.829+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-11-09T15:16:37.829+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-11-09T15:16:37.840+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
[2025-11-09T15:16:37.867+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-11-09T15:16:37.896+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
[2025-11-09T15:16:37.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Registering RDD 31 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 1
[2025-11-09T15:16:37.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Got job 7 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
[2025-11-09T15:16:37.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Final stage: ResultStage 9 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
[2025-11-09T15:16:37.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-11-09T15:16:37.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)
[2025-11-09T15:16:37.902+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[31] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
[2025-11-09T15:16:37.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.5 KiB, free 433.8 MiB)
[2025-11-09T15:16:37.917+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.7 MiB)
[2025-11-09T15:16:37.918+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 863c17993851:44651 (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:37.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:37.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[31] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:37.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-11-09T15:16:37.924+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7503 bytes)
[2025-11-09T15:16:37.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on 172.19.0.9:35743 (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:37.994+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 72 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:37.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-11-09T15:16:37.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: ShuffleMapStage 8 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.093 s
[2025-11-09T15:16:37.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:37.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:37.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: waiting: Set(ResultStage 9)
[2025-11-09T15:16:37.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:37.996+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[34] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
[2025-11-09T15:16:37.999+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:37 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.3 KiB, free 433.7 MiB)
[2025-11-09T15:16:38.005+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.7 MiB)
[2025-11-09T15:16:38.006+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 863c17993851:44651 (size: 3.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:38.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:38.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[34] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:38.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-11-09T15:16:38.009+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:38.030+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on 172.19.0.9:35743 (size: 3.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:38.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.0.9:56384
[2025-11-09T15:16:38.083+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 74 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:38.084+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-11-09T15:16:38.085+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: ResultStage 9 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.087 s
[2025-11-09T15:16:38.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:38.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-11-09T15:16:38.087+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Job 7 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.190128 s
[2025-11-09T15:16:38.153+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-11-09T15:16:38.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
[2025-11-09T15:16:38.210+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-11-09T15:16:38.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Got job 8 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-11-09T15:16:38.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HoodieJavaRDD.java:177)
[2025-11-09T15:16:38.212+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-11-09T15:16:38.216+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:38.219+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[37] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-11-09T15:16:38.236+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 357.5 KiB, free 433.4 MiB)
[2025-11-09T15:16:38.243+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 123.6 KiB, free 433.3 MiB)
[2025-11-09T15:16:38.244+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 863c17993851:44651 (size: 123.6 KiB, free: 434.1 MiB)
[2025-11-09T15:16:38.245+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:38.245+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[37] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:38.246+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-11-09T15:16:38.248+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:38.265+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on 172.19.0.9:35743 (size: 123.6 KiB, free: 434.1 MiB)
[2025-11-09T15:16:38.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Added rdd_36_0 in memory on 172.19.0.9:35743 (size: 290.0 B, free: 434.1 MiB)
[2025-11-09T15:16:38.918+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 670 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:38.918+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-11-09T15:16:38.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: ResultStage 11 (collect at HoodieJavaRDD.java:177) finished in 0.700 s
[2025-11-09T15:16:38.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:38.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-11-09T15:16:38.921+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Job 8 finished: collect at HoodieJavaRDD.java:177, took 0.709950 s
[2025-11-09T15:16:38.922+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
[2025-11-09T15:16:38.923+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
[2025-11-09T15:16:38.971+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 863c17993851:44651 in memory (size: 36.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:38.972+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-11-09T15:16:38.973+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Got job 9 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-11-09T15:16:38.973+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Final stage: ResultStage 12 (collect at HoodieSparkEngineContext.java:150)
[2025-11-09T15:16:38.974+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:38.974+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:38.974+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-11-09T15:16:38.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Removed broadcast_4_piece0 on 172.19.0.9:35743 in memory (size: 36.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:38.978+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 100.7 KiB, free 433.3 MiB)
[2025-11-09T15:16:38.983+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.3 MiB)
[2025-11-09T15:16:38.985+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 863c17993851:44651 (size: 35.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:38.987+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:38.987+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:38.987+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 863c17993851:44651 in memory (size: 123.6 KiB, free: 434.2 MiB)
[2025-11-09T15:16:38.987+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-11-09T15:16:38.989+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-11-09T15:16:38.990+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:38 INFO BlockManagerInfo: Removed broadcast_9_piece0 on 172.19.0.9:35743 in memory (size: 123.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:39.001+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 863c17993851:44651 in memory (size: 3.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:39.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_8_piece0 on 172.19.0.9:35743 in memory (size: 3.9 KiB, free: 434.3 MiB)
[2025-11-09T15:16:39.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on 172.19.0.9:35743 (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:39.017+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 863c17993851:44651 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:39.020+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_7_piece0 on 172.19.0.9:35743 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:39.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 863c17993851:44651 in memory (size: 1866.0 B, free: 434.2 MiB)
[2025-11-09T15:16:39.034+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_5_piece0 on 172.19.0.9:35743 in memory (size: 1866.0 B, free: 434.2 MiB)
[2025-11-09T15:16:39.047+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 58 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:39.048+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-11-09T15:16:39.049+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: ResultStage 12 (collect at HoodieSparkEngineContext.java:150) finished in 0.073 s
[2025-11-09T15:16:39.049+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:39.051+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-11-09T15:16:39.051+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 863c17993851:44651 in memory (size: 126.6 KiB, free: 434.4 MiB)
[2025-11-09T15:16:39.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Job 9 finished: collect at HoodieSparkEngineContext.java:150, took 0.079182 s
[2025-11-09T15:16:39.053+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Removed broadcast_6_piece0 on 172.19.0.9:35743 in memory (size: 126.6 KiB, free: 434.4 MiB)
[2025-11-09T15:16:39.080+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
[2025-11-09T15:16:39.080+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-11-09T15:16:39.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
[2025-11-09T15:16:39.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
[2025-11-09T15:16:39.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
[2025-11-09T15:16:39.151+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-11-09T15:16:39.152+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Got job 10 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-11-09T15:16:39.152+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Final stage: ResultStage 13 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-11-09T15:16:39.152+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:39.152+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:39.153+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-11-09T15:16:39.157+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 100.9 KiB, free 434.2 MiB)
[2025-11-09T15:16:39.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.1 MiB)
[2025-11-09T15:16:39.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 863c17993851:44651 (size: 36.0 KiB, free: 434.3 MiB)
[2025-11-09T15:16:39.163+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:39.163+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:39.164+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-11-09T15:16:39.165+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-11-09T15:16:39.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on 172.19.0.9:35743 (size: 36.0 KiB, free: 434.3 MiB)
[2025-11-09T15:16:39.249+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 84 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:39.249+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-11-09T15:16:39.250+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: ResultStage 13 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.096 s
[2025-11-09T15:16:39.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:39.253+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-11-09T15:16:39.253+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Job 10 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.101293 s
[2025-11-09T15:16:39.295+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/.temp/00000000000000010
[2025-11-09T15:16:39.295+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.304+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.313+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.314+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.321+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.331+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.340+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.340+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.349+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.357+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.463+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: MDT s3a://huditest/silver/table_name=customers partition FILES has been enabled
[2025-11-09T15:16:39.463+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.470+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:39.477+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.477+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.483+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.491+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:39.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:39.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 2437 in ms
[2025-11-09T15:16:39.498+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.522+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.530+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.530+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.531+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.538+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.545+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.546+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.552+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.558+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.564+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.565+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.572+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.573+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
[2025-11-09T15:16:39.578+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.579+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.587+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.593+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.593+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.598+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.606+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.612+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.612+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.613+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BaseHoodieWriteClient: Cleaner started
[2025-11-09T15:16:39.613+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.620+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.626+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.626+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.630+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.638+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.644+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.644+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.645+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
[2025-11-09T15:16:39.649+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.650+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:39.650+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:39.653+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
[2025-11-09T15:16:39.653+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
[2025-11-09T15:16:39.677+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.680+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.687+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.693+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.694+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.701+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.712+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.713+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.719+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:39.720+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
[2025-11-09T15:16:39.720+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.728+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.735+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.735+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.741+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.750+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.757+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.757+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.758+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
[2025-11-09T15:16:39.762+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.765+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.772+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.777+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.778+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:39.784+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.791+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.796+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:39.796+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:39.814+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.814+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.826+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:39.837+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.844+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:39.846+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTimelineArchiver: No Instants to archive
[2025-11-09T15:16:39.847+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
[2025-11-09T15:16:39.848+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.858+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:39.865+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.866+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.874+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.880+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.886+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.886+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:39.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:39.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:39.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:39.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO AsyncCleanerService: Starting async clean service with instant time 20251109151639887...
[2025-11-09T15:16:39.888+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.890+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-11-09T15:16:39.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:39.912+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:39.921+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.928+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:39.936+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.937+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.944+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:39.951+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-11-09T15:16:39.953+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:39.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:39.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Registering RDD 42 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 3
[2025-11-09T15:16:39.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:39.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Registering RDD 46 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 2
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BaseHoodieWriteClient: Cleaner started
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Got job 11 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Final stage: ResultStage 16 (countByKey at HoodieJavaPairRDD.java:105)
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
[2025-11-09T15:16:39.962+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[42] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:39.972+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:39.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 76.9 KiB, free 434.1 MiB)
[2025-11-09T15:16:39.978+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.6 KiB, free 434.0 MiB)
[2025-11-09T15:16:39.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 863c17993851:44651 (size: 27.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:39.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.981+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:39.981+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:39.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[42] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:39.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2025-11-09T15:16:39.986+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 11842 bytes)
[2025-11-09T15:16:39.986+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 14) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 11856 bytes)
[2025-11-09T15:16:39.991+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:39.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:39 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:40.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:40.008+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on 172.19.0.9:35743 (size: 27.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.016+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:40.017+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:40.027+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:40.036+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:40.044+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:40.045+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:40.045+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:40.046+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:40.046+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:40.046+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :20251109151639887
[2025-11-09T15:16:40.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__REQUESTED__20251109151635023]}
[2025-11-09T15:16:40.219+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 234 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:40.227+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 14) in 241 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:40.227+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-11-09T15:16:40.228+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: ShuffleMapStage 14 (mapToPair at HoodieJavaRDD.java:149) finished in 0.265 s
[2025-11-09T15:16:40.229+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:40.229+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:40.229+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16)
[2025-11-09T15:16:40.229+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:40.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:40.234+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 27.1 KiB, free 434.0 MiB)
[2025-11-09T15:16:40.241+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 434.0 MiB)
[2025-11-09T15:16:40.243+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 863c17993851:44651 (size: 12.9 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.243+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:40.243+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:40.244+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks resource profile 0
[2025-11-09T15:16:40.245+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:40.245+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 16) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:40.262+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on 172.19.0.9:35743 (size: 12.9 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.279+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.19.0.9:56384
[2025-11-09T15:16:40.306+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added rdd_44_0 in memory on 172.19.0.9:35743 (size: 3.2 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.306+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added rdd_44_1 in memory on 172.19.0.9:35743 (size: 4.9 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.323+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 78 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:40.327+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 16) in 82 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:40.327+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-11-09T15:16:40.328+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: ShuffleMapStage 15 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.096 s
[2025-11-09T15:16:40.328+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:40.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:40.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: waiting: Set(ResultStage 16)
[2025-11-09T15:16:40.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:40.330+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting ResultStage 16 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:40.332+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.5 KiB, free 434.0 MiB)
[2025-11-09T15:16:40.341+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.0 MiB)
[2025-11-09T15:16:40.342+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.343+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:40.343+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:40.343+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0
[2025-11-09T15:16:40.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 17) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:40.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 18) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:40.360+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.367+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.19.0.9:56384
[2025-11-09T15:16:40.389+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 18) in 43 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:40.394+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 17) in 48 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:40.394+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-11-09T15:16:40.395+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: ResultStage 16 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.064 s
[2025-11-09T15:16:40.395+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:40.396+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-11-09T15:16:40.396+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Job 11 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.444461 s
[2025-11-09T15:16:40.465+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-11-09T15:16:40.466+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Got job 12 (collect at HoodieSparkEngineContext.java:150) with 31 output partitions
[2025-11-09T15:16:40.467+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Final stage: ResultStage 17 (collect at HoodieSparkEngineContext.java:150)
[2025-11-09T15:16:40.467+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:40.467+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:40.468+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[49] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-11-09T15:16:40.498+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 453.7 KiB, free 433.5 MiB)
[2025-11-09T15:16:40.501+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.504+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_14_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.508+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 158.2 KiB, free 433.4 MiB)
[2025-11-09T15:16:40.509+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 863c17993851:44651 (size: 158.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:40.509+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:40.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 17 (MapPartitionsRDD[49] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-11-09T15:16:40.511+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Adding task set 17.0 with 31 tasks resource profile 0
[2025-11-09T15:16:40.513+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 19) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:40.514+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 20) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7263 bytes)
[2025-11-09T15:16:40.514+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 21) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:40.514+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 22) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7258 bytes)
[2025-11-09T15:16:40.517+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 863c17993851:44651 in memory (size: 27.6 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.519+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_12_piece0 on 172.19.0.9:35743 in memory (size: 27.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:40.531+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on 172.19.0.9:35743 (size: 158.2 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.532+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 863c17993851:44651 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.533+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_10_piece0 on 172.19.0.9:35743 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.550+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManager: Removing RDD 36
[2025-11-09T15:16:40.569+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 863c17993851:44651 in memory (size: 12.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.571+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_13_piece0 on 172.19.0.9:35743 in memory (size: 12.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.582+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 863c17993851:44651 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.583+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Removed broadcast_11_piece0 on 172.19.0.9:35743 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:40.606+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 4.0 in stage 17.0 (TID 23) (172.19.0.9, executor 0, partition 4, PROCESS_LOCAL, 7265 bytes)
[2025-11-09T15:16:40.607+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 22) in 92 ms on 172.19.0.9 (executor 0) (1/31)
[2025-11-09T15:16:40.612+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 5.0 in stage 17.0 (TID 24) (172.19.0.9, executor 0, partition 5, PROCESS_LOCAL, 7266 bytes)
[2025-11-09T15:16:40.613+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 20) in 99 ms on 172.19.0.9 (executor 0) (2/31)
[2025-11-09T15:16:40.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 6.0 in stage 17.0 (TID 25) (172.19.0.9, executor 0, partition 6, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 19) in 107 ms on 172.19.0.9 (executor 0) (3/31)
[2025-11-09T15:16:40.625+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 7.0 in stage 17.0 (TID 26) (172.19.0.9, executor 0, partition 7, PROCESS_LOCAL, 7268 bytes)
[2025-11-09T15:16:40.626+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 21) in 113 ms on 172.19.0.9 (executor 0) (4/31)
[2025-11-09T15:16:40.637+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 8.0 in stage 17.0 (TID 27) (172.19.0.9, executor 0, partition 8, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:40.638+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 4.0 in stage 17.0 (TID 23) in 31 ms on 172.19.0.9 (executor 0) (5/31)
[2025-11-09T15:16:40.647+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 9.0 in stage 17.0 (TID 28) (172.19.0.9, executor 0, partition 9, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.648+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 5.0 in stage 17.0 (TID 24) in 40 ms on 172.19.0.9 (executor 0) (6/31)
[2025-11-09T15:16:40.654+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 10.0 in stage 17.0 (TID 29) (172.19.0.9, executor 0, partition 10, PROCESS_LOCAL, 7259 bytes)
[2025-11-09T15:16:40.655+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 6.0 in stage 17.0 (TID 25) in 41 ms on 172.19.0.9 (executor 0) (7/31)
[2025-11-09T15:16:40.661+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 11.0 in stage 17.0 (TID 30) (172.19.0.9, executor 0, partition 11, PROCESS_LOCAL, 7266 bytes)
[2025-11-09T15:16:40.662+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 7.0 in stage 17.0 (TID 26) in 42 ms on 172.19.0.9 (executor 0) (8/31)
[2025-11-09T15:16:40.667+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 12.0 in stage 17.0 (TID 31) (172.19.0.9, executor 0, partition 12, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.668+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 8.0 in stage 17.0 (TID 27) in 36 ms on 172.19.0.9 (executor 0) (9/31)
[2025-11-09T15:16:40.677+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 13.0 in stage 17.0 (TID 32) (172.19.0.9, executor 0, partition 13, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.677+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 9.0 in stage 17.0 (TID 28) in 34 ms on 172.19.0.9 (executor 0) (10/31)
[2025-11-09T15:16:40.684+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 14.0 in stage 17.0 (TID 33) (172.19.0.9, executor 0, partition 14, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 10.0 in stage 17.0 (TID 29) in 37 ms on 172.19.0.9 (executor 0) (11/31)
[2025-11-09T15:16:40.693+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 15.0 in stage 17.0 (TID 34) (172.19.0.9, executor 0, partition 15, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:40.694+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 12.0 in stage 17.0 (TID 31) in 30 ms on 172.19.0.9 (executor 0) (12/31)
[2025-11-09T15:16:40.711+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 16.0 in stage 17.0 (TID 35) (172.19.0.9, executor 0, partition 16, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:40.712+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 11.0 in stage 17.0 (TID 30) in 56 ms on 172.19.0.9 (executor 0) (13/31)
[2025-11-09T15:16:40.716+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 17.0 in stage 17.0 (TID 36) (172.19.0.9, executor 0, partition 17, PROCESS_LOCAL, 7259 bytes)
[2025-11-09T15:16:40.722+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 18.0 in stage 17.0 (TID 37) (172.19.0.9, executor 0, partition 18, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.722+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 13.0 in stage 17.0 (TID 32) in 49 ms on 172.19.0.9 (executor 0) (14/31)
[2025-11-09T15:16:40.723+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 14.0 in stage 17.0 (TID 33) in 41 ms on 172.19.0.9 (executor 0) (15/31)
[2025-11-09T15:16:40.727+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 19.0 in stage 17.0 (TID 38) (172.19.0.9, executor 0, partition 19, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:40.728+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 15.0 in stage 17.0 (TID 34) in 38 ms on 172.19.0.9 (executor 0) (16/31)
[2025-11-09T15:16:40.737+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 20.0 in stage 17.0 (TID 39) (172.19.0.9, executor 0, partition 20, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:40.738+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 16.0 in stage 17.0 (TID 35) in 45 ms on 172.19.0.9 (executor 0) (17/31)
[2025-11-09T15:16:40.744+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 21.0 in stage 17.0 (TID 40) (172.19.0.9, executor 0, partition 21, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:40.744+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 17.0 in stage 17.0 (TID 36) in 31 ms on 172.19.0.9 (executor 0) (18/31)
[2025-11-09T15:16:40.752+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 22.0 in stage 17.0 (TID 41) (172.19.0.9, executor 0, partition 22, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.753+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 18.0 in stage 17.0 (TID 37) in 36 ms on 172.19.0.9 (executor 0) (19/31)
[2025-11-09T15:16:40.761+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 23.0 in stage 17.0 (TID 42) (172.19.0.9, executor 0, partition 23, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:40.761+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 19.0 in stage 17.0 (TID 38) in 38 ms on 172.19.0.9 (executor 0) (20/31)
[2025-11-09T15:16:40.773+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 24.0 in stage 17.0 (TID 43) (172.19.0.9, executor 0, partition 24, PROCESS_LOCAL, 7263 bytes)
[2025-11-09T15:16:40.774+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 20.0 in stage 17.0 (TID 39) in 41 ms on 172.19.0.9 (executor 0) (21/31)
[2025-11-09T15:16:40.780+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 25.0 in stage 17.0 (TID 44) (172.19.0.9, executor 0, partition 25, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:40.780+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 22.0 in stage 17.0 (TID 41) in 35 ms on 172.19.0.9 (executor 0) (22/31)
[2025-11-09T15:16:40.787+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 26.0 in stage 17.0 (TID 45) (172.19.0.9, executor 0, partition 26, PROCESS_LOCAL, 7264 bytes)
[2025-11-09T15:16:40.788+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 21.0 in stage 17.0 (TID 40) in 48 ms on 172.19.0.9 (executor 0) (23/31)
[2025-11-09T15:16:40.793+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 27.0 in stage 17.0 (TID 46) (172.19.0.9, executor 0, partition 27, PROCESS_LOCAL, 7267 bytes)
[2025-11-09T15:16:40.794+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 23.0 in stage 17.0 (TID 42) in 37 ms on 172.19.0.9 (executor 0) (24/31)
[2025-11-09T15:16:40.804+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 28.0 in stage 17.0 (TID 47) (172.19.0.9, executor 0, partition 28, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:40.806+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 24.0 in stage 17.0 (TID 43) in 36 ms on 172.19.0.9 (executor 0) (25/31)
[2025-11-09T15:16:40.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 29.0 in stage 17.0 (TID 48) (172.19.0.9, executor 0, partition 29, PROCESS_LOCAL, 7266 bytes)
[2025-11-09T15:16:40.814+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 25.0 in stage 17.0 (TID 44) in 38 ms on 172.19.0.9 (executor 0) (26/31)
[2025-11-09T15:16:40.821+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 30.0 in stage 17.0 (TID 49) (172.19.0.9, executor 0, partition 30, PROCESS_LOCAL, 7263 bytes)
[2025-11-09T15:16:40.822+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 27.0 in stage 17.0 (TID 46) in 33 ms on 172.19.0.9 (executor 0) (27/31)
[2025-11-09T15:16:40.826+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 26.0 in stage 17.0 (TID 45) in 45 ms on 172.19.0.9 (executor 0) (28/31)
[2025-11-09T15:16:40.834+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 28.0 in stage 17.0 (TID 47) in 34 ms on 172.19.0.9 (executor 0) (29/31)
[2025-11-09T15:16:40.847+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 29.0 in stage 17.0 (TID 48) in 38 ms on 172.19.0.9 (executor 0) (30/31)
[2025-11-09T15:16:40.851+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 30.0 in stage 17.0 (TID 49) in 36 ms on 172.19.0.9 (executor 0) (31/31)
[2025-11-09T15:16:40.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-11-09T15:16:40.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: ResultStage 17 (collect at HoodieSparkEngineContext.java:150) finished in 0.383 s
[2025-11-09T15:16:40.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:40.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-11-09T15:16:40.854+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Job 12 finished: collect at HoodieSparkEngineContext.java:150, took 0.388005 s
[2025-11-09T15:16:40.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-11-09T15:16:40.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Got job 13 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-11-09T15:16:40.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Final stage: ResultStage 18 (collect at HoodieSparkEngineContext.java:116)
[2025-11-09T15:16:40.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:40.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:40.910+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[51] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-11-09T15:16:40.926+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 453.4 KiB, free 433.4 MiB)
[2025-11-09T15:16:40.931+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 158.1 KiB, free 433.2 MiB)
[2025-11-09T15:16:40.932+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 863c17993851:44651 (size: 158.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:40.933+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:40.934+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[51] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:40.934+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-11-09T15:16:40.936+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 50) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7246 bytes)
[2025-11-09T15:16:40.954+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on 172.19.0.9:35743 (size: 158.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:40.978+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 50) in 43 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:40.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-11-09T15:16:40.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: ResultStage 18 (collect at HoodieSparkEngineContext.java:116) finished in 0.069 s
[2025-11-09T15:16:40.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:40.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-11-09T15:16:40.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO DAGScheduler: Job 13 finished: collect at HoodieSparkEngineContext.java:116, took 0.073044 s
[2025-11-09T15:16:40.993+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:40 INFO SparkHoodieBloomIndexHelper: Input parallelism: 2, Index parallelism: 2
[2025-11-09T15:16:41.001+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Starting job: countByKey at SparkHoodieBloomIndexHelper.java:197
[2025-11-09T15:16:41.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Registering RDD 54 (countByKey at SparkHoodieBloomIndexHelper.java:197) as input to shuffle 4
[2025-11-09T15:16:41.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Got job 14 (countByKey at SparkHoodieBloomIndexHelper.java:197) with 2 output partitions
[2025-11-09T15:16:41.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Final stage: ResultStage 21 (countByKey at SparkHoodieBloomIndexHelper.java:197)
[2025-11-09T15:16:41.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
[2025-11-09T15:16:41.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
[2025-11-09T15:16:41.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[54] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-11-09T15:16:41.009+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 28.8 KiB, free 433.2 MiB)
[2025-11-09T15:16:41.013+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 433.2 MiB)
[2025-11-09T15:16:41.013+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 863c17993851:44651 (size: 13.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.014+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.014+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[54] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:41.015+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0
[2025-11-09T15:16:41.017+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 51) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7174 bytes)
[2025-11-09T15:16:41.017+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 52) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7174 bytes)
[2025-11-09T15:16:41.035+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on 172.19.0.9:35743 (size: 13.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.060+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 52) in 43 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:41.063+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 51) in 47 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:41.063+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-11-09T15:16:41.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: ShuffleMapStage 20 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.056 s
[2025-11-09T15:16:41.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:41.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:41.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: waiting: Set(ResultStage 21)
[2025-11-09T15:16:41.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:41.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRDD[55] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-11-09T15:16:41.066+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 5.5 KiB, free 433.2 MiB)
[2025-11-09T15:16:41.069+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.2 MiB)
[2025-11-09T15:16:41.069+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 21 (ShuffledRDD[55] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:41.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0
[2025-11-09T15:16:41.072+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 53) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:41.072+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 54) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:41.087+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.094+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.19.0.9:56384
[2025-11-09T15:16:41.105+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 54) in 32 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:41.107+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 53) in 35 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:41.107+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-11-09T15:16:41.107+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: ResultStage 21 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.043 s
[2025-11-09T15:16:41.108+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:41.108+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2025-11-09T15:16:41.108+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Job 14 finished: countByKey at SparkHoodieBloomIndexHelper.java:197, took 0.107140 s
[2025-11-09T15:16:41.110+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BucketizedBloomCheckPartitioner: TotalBuckets 0, min_buckets/partition 1
[2025-11-09T15:16:41.158+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapPartitionsRDD: Removing RDD 44 from persistence list
[2025-11-09T15:16:41.159+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapPartitionsRDD: Removing RDD 62 from persistence list
[2025-11-09T15:16:41.160+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManager: Removing RDD 44
[2025-11-09T15:16:41.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:41.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManager: Removing RDD 62
[2025-11-09T15:16:41.170+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:41.183+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-11-09T15:16:41.186+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Registering RDD 56 (mapToPair at SparkHoodieBloomIndexHelper.java:166) as input to shuffle 8
[2025-11-09T15:16:41.186+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Registering RDD 62 (flatMapToPair at SparkHoodieBloomIndexHelper.java:177) as input to shuffle 6
[2025-11-09T15:16:41.186+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Registering RDD 63 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 5
[2025-11-09T15:16:41.187+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Registering RDD 72 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 7
[2025-11-09T15:16:41.188+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Got job 15 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
[2025-11-09T15:16:41.188+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Final stage: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105)
[2025-11-09T15:16:41.188+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
[2025-11-09T15:16:41.188+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
[2025-11-09T15:16:41.190+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[63] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:41.192+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 26.8 KiB, free 433.1 MiB)
[2025-11-09T15:16:41.196+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.1 MiB)
[2025-11-09T15:16:41.197+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 863c17993851:44651 (size: 12.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.198+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.202+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[63] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:41.202+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks resource profile 0
[2025-11-09T15:16:41.203+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 55) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:41.203+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 56) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:41.220+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on 172.19.0.9:35743 (size: 12.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.228+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.19.0.9:56384
[2025-11-09T15:16:41.249+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 56) in 45 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:41.251+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 55) in 48 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:41.251+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-11-09T15:16:41.251+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: ShuffleMapStage 25 (mapToPair at HoodieJavaRDD.java:149) finished in 0.061 s
[2025-11-09T15:16:41.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:41.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:41.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: waiting: Set(ResultStage 27, ShuffleMapStage 26)
[2025-11-09T15:16:41.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:41.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[72] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:41.255+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 10.0 KiB, free 433.1 MiB)
[2025-11-09T15:16:41.271+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.1 MiB)
[2025-11-09T15:16:41.273+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 863c17993851:44651 (size: 5.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.274+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 863c17993851:44651 in memory (size: 158.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.274+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.275+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[72] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:41.276+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 26.0 with 2 tasks resource profile 0
[2025-11-09T15:16:41.276+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_16_piece0 on 172.19.0.9:35743 in memory (size: 158.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.279+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 57) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:41.279+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 58) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:41.310+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.312+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_18_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.317+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on 172.19.0.9:35743 (size: 5.2 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.323+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 863c17993851:44651 in memory (size: 13.5 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.325+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_17_piece0 on 172.19.0.9:35743 in memory (size: 13.5 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.336+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 863c17993851:44651 in memory (size: 158.2 KiB, free: 434.4 MiB)
[2025-11-09T15:16:41.337+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Removed broadcast_15_piece0 on 172.19.0.9:35743 in memory (size: 158.2 KiB, free: 434.4 MiB)
[2025-11-09T15:16:41.338+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.19.0.9:56384
[2025-11-09T15:16:41.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.19.0.9:56384
[2025-11-09T15:16:41.369+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added rdd_70_1 in memory on 172.19.0.9:35743 (size: 4.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:41.370+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added rdd_70_0 in memory on 172.19.0.9:35743 (size: 3.2 KiB, free: 434.4 MiB)
[2025-11-09T15:16:41.380+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 58) in 100 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:41.382+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 57) in 105 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:41.382+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-11-09T15:16:41.383+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: ShuffleMapStage 26 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.130 s
[2025-11-09T15:16:41.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:41.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:41.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: waiting: Set(ResultStage 27)
[2025-11-09T15:16:41.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:41.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ResultStage 27 (ShuffledRDD[73] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:41.386+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 5.5 KiB, free 434.3 MiB)
[2025-11-09T15:16:41.390+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:41.391+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:41.392+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.392+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 27 (ShuffledRDD[73] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:41.392+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0
[2025-11-09T15:16:41.394+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 59) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:41.394+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 60) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:41.411+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:41.417+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.19.0.9:56384
[2025-11-09T15:16:41.442+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 59) in 48 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:41.443+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 60) in 48 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:41.443+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-11-09T15:16:41.443+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.058 s
[2025-11-09T15:16:41.443+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:41.444+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2025-11-09T15:16:41.444+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Job 15 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.261095 s
[2025-11-09T15:16:41.447+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-11-09T15:16:41.491+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-11-09T15:16:41.493+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Got job 16 (collectAsMap at UpsertPartitioner.java:282) with 31 output partitions
[2025-11-09T15:16:41.493+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Final stage: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282)
[2025-11-09T15:16:41.493+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:41.493+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:41.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[75] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-11-09T15:16:41.511+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 457.0 KiB, free 433.9 MiB)
[2025-11-09T15:16:41.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 159.1 KiB, free 433.7 MiB)
[2025-11-09T15:16:41.517+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 863c17993851:44651 (size: 159.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.518+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.518+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 28 (MapPartitionsRDD[75] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-11-09T15:16:41.519+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 28.0 with 31 tasks resource profile 0
[2025-11-09T15:16:41.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 61) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:41.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 62) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7267 bytes)
[2025-11-09T15:16:41.521+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 63) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7266 bytes)
[2025-11-09T15:16:41.521+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 64) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:41.537+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on 172.19.0.9:35743 (size: 159.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:41.581+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 4.0 in stage 28.0 (TID 65) (172.19.0.9, executor 0, partition 4, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:41.582+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 64) in 61 ms on 172.19.0.9 (executor 0) (1/31)
[2025-11-09T15:16:41.587+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 5.0 in stage 28.0 (TID 66) (172.19.0.9, executor 0, partition 5, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.592+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 6.0 in stage 28.0 (TID 67) (172.19.0.9, executor 0, partition 6, PROCESS_LOCAL, 7263 bytes)
[2025-11-09T15:16:41.593+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 61) in 72 ms on 172.19.0.9 (executor 0) (2/31)
[2025-11-09T15:16:41.593+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 62) in 72 ms on 172.19.0.9 (executor 0) (3/31)
[2025-11-09T15:16:41.597+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 7.0 in stage 28.0 (TID 68) (172.19.0.9, executor 0, partition 7, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.598+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 63) in 78 ms on 172.19.0.9 (executor 0) (4/31)
[2025-11-09T15:16:41.619+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 8.0 in stage 28.0 (TID 69) (172.19.0.9, executor 0, partition 8, PROCESS_LOCAL, 7265 bytes)
[2025-11-09T15:16:41.620+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 4.0 in stage 28.0 (TID 65) in 39 ms on 172.19.0.9 (executor 0) (5/31)
[2025-11-09T15:16:41.628+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 9.0 in stage 28.0 (TID 70) (172.19.0.9, executor 0, partition 9, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:41.630+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 5.0 in stage 28.0 (TID 66) in 46 ms on 172.19.0.9 (executor 0) (6/31)
[2025-11-09T15:16:41.637+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 10.0 in stage 28.0 (TID 71) (172.19.0.9, executor 0, partition 10, PROCESS_LOCAL, 7264 bytes)
[2025-11-09T15:16:41.639+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 6.0 in stage 28.0 (TID 67) in 50 ms on 172.19.0.9 (executor 0) (7/31)
[2025-11-09T15:16:41.644+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 11.0 in stage 28.0 (TID 72) (172.19.0.9, executor 0, partition 11, PROCESS_LOCAL, 7268 bytes)
[2025-11-09T15:16:41.645+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 7.0 in stage 28.0 (TID 68) in 52 ms on 172.19.0.9 (executor 0) (8/31)
[2025-11-09T15:16:41.657+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 12.0 in stage 28.0 (TID 73) (172.19.0.9, executor 0, partition 12, PROCESS_LOCAL, 7266 bytes)
[2025-11-09T15:16:41.662+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 13.0 in stage 28.0 (TID 74) (172.19.0.9, executor 0, partition 13, PROCESS_LOCAL, 7266 bytes)
[2025-11-09T15:16:41.663+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 9.0 in stage 28.0 (TID 70) in 38 ms on 172.19.0.9 (executor 0) (9/31)
[2025-11-09T15:16:41.663+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 8.0 in stage 28.0 (TID 69) in 48 ms on 172.19.0.9 (executor 0) (10/31)
[2025-11-09T15:16:41.666+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 14.0 in stage 28.0 (TID 75) (172.19.0.9, executor 0, partition 14, PROCESS_LOCAL, 7259 bytes)
[2025-11-09T15:16:41.671+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 15.0 in stage 28.0 (TID 76) (172.19.0.9, executor 0, partition 15, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.672+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 10.0 in stage 28.0 (TID 71) in 39 ms on 172.19.0.9 (executor 0) (11/31)
[2025-11-09T15:16:41.672+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 11.0 in stage 28.0 (TID 72) in 33 ms on 172.19.0.9 (executor 0) (12/31)
[2025-11-09T15:16:41.688+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 16.0 in stage 28.0 (TID 77) (172.19.0.9, executor 0, partition 16, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:41.689+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 12.0 in stage 28.0 (TID 73) in 35 ms on 172.19.0.9 (executor 0) (13/31)
[2025-11-09T15:16:41.693+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 17.0 in stage 28.0 (TID 78) (172.19.0.9, executor 0, partition 17, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:41.694+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 13.0 in stage 28.0 (TID 74) in 35 ms on 172.19.0.9 (executor 0) (14/31)
[2025-11-09T15:16:41.700+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 18.0 in stage 28.0 (TID 79) (172.19.0.9, executor 0, partition 18, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.704+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 19.0 in stage 28.0 (TID 80) (172.19.0.9, executor 0, partition 19, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:41.705+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 15.0 in stage 28.0 (TID 76) in 38 ms on 172.19.0.9 (executor 0) (15/31)
[2025-11-09T15:16:41.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 14.0 in stage 28.0 (TID 75) in 42 ms on 172.19.0.9 (executor 0) (16/31)
[2025-11-09T15:16:41.714+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 20.0 in stage 28.0 (TID 81) (172.19.0.9, executor 0, partition 20, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.715+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 16.0 in stage 28.0 (TID 77) in 32 ms on 172.19.0.9 (executor 0) (17/31)
[2025-11-09T15:16:41.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 21.0 in stage 28.0 (TID 82) (172.19.0.9, executor 0, partition 21, PROCESS_LOCAL, 7258 bytes)
[2025-11-09T15:16:41.725+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 22.0 in stage 28.0 (TID 83) (172.19.0.9, executor 0, partition 22, PROCESS_LOCAL, 7263 bytes)
[2025-11-09T15:16:41.725+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 17.0 in stage 28.0 (TID 78) in 36 ms on 172.19.0.9 (executor 0) (18/31)
[2025-11-09T15:16:41.725+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 18.0 in stage 28.0 (TID 79) in 30 ms on 172.19.0.9 (executor 0) (19/31)
[2025-11-09T15:16:41.729+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 23.0 in stage 28.0 (TID 84) (172.19.0.9, executor 0, partition 23, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.730+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 19.0 in stage 28.0 (TID 80) in 28 ms on 172.19.0.9 (executor 0) (20/31)
[2025-11-09T15:16:41.734+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 24.0 in stage 28.0 (TID 85) (172.19.0.9, executor 0, partition 24, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.735+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 20.0 in stage 28.0 (TID 81) in 25 ms on 172.19.0.9 (executor 0) (21/31)
[2025-11-09T15:16:41.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 25.0 in stage 28.0 (TID 86) (172.19.0.9, executor 0, partition 25, PROCESS_LOCAL, 7262 bytes)
[2025-11-09T15:16:41.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 21.0 in stage 28.0 (TID 82) in 25 ms on 172.19.0.9 (executor 0) (22/31)
[2025-11-09T15:16:41.745+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 26.0 in stage 28.0 (TID 87) (172.19.0.9, executor 0, partition 26, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:41.748+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 27.0 in stage 28.0 (TID 88) (172.19.0.9, executor 0, partition 27, PROCESS_LOCAL, 7259 bytes)
[2025-11-09T15:16:41.749+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 22.0 in stage 28.0 (TID 83) in 26 ms on 172.19.0.9 (executor 0) (23/31)
[2025-11-09T15:16:41.749+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 23.0 in stage 28.0 (TID 84) in 23 ms on 172.19.0.9 (executor 0) (24/31)
[2025-11-09T15:16:41.757+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 28.0 in stage 28.0 (TID 89) (172.19.0.9, executor 0, partition 28, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:41.758+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 24.0 in stage 28.0 (TID 85) in 27 ms on 172.19.0.9 (executor 0) (25/31)
[2025-11-09T15:16:41.766+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 29.0 in stage 28.0 (TID 90) (172.19.0.9, executor 0, partition 29, PROCESS_LOCAL, 7263 bytes)
[2025-11-09T15:16:41.772+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 30.0 in stage 28.0 (TID 91) (172.19.0.9, executor 0, partition 30, PROCESS_LOCAL, 7261 bytes)
[2025-11-09T15:16:41.772+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 25.0 in stage 28.0 (TID 86) in 33 ms on 172.19.0.9 (executor 0) (26/31)
[2025-11-09T15:16:41.773+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 27.0 in stage 28.0 (TID 88) in 27 ms on 172.19.0.9 (executor 0) (27/31)
[2025-11-09T15:16:41.775+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 26.0 in stage 28.0 (TID 87) in 32 ms on 172.19.0.9 (executor 0) (28/31)
[2025-11-09T15:16:41.776+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 28.0 in stage 28.0 (TID 89) in 22 ms on 172.19.0.9 (executor 0) (29/31)
[2025-11-09T15:16:41.787+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 29.0 in stage 28.0 (TID 90) in 24 ms on 172.19.0.9 (executor 0) (30/31)
[2025-11-09T15:16:41.794+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Finished task 30.0 in stage 28.0 (TID 91) in 26 ms on 172.19.0.9 (executor 0) (31/31)
[2025-11-09T15:16:41.794+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-11-09T15:16:41.794+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282) finished in 0.296 s
[2025-11-09T15:16:41.795+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:41.795+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2025-11-09T15:16:41.795+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Job 16 finished: collectAsMap at UpsertPartitioner.java:282, took 0.303983 s
[2025-11-09T15:16:41.796+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:41.805+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:41.808+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO UpsertPartitioner: Total Buckets: 31
[2025-11-09T15:16:41.811+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/20251109151634225.commit.requested
[2025-11-09T15:16:41.845+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/20251109151634225.inflight
[2025-11-09T15:16:41.873+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-11-09T15:16:41.874+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BaseCommitActionExecutor: Auto commit disabled for 20251109151634225
[2025-11-09T15:16:41.884+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1050
[2025-11-09T15:16:41.886+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Registering RDD 76 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 9
[2025-11-09T15:16:41.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Got job 17 (count at HoodieSparkSqlWriter.scala:1050) with 31 output partitions
[2025-11-09T15:16:41.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Final stage: ResultStage 34 (count at HoodieSparkSqlWriter.scala:1050)
[2025-11-09T15:16:41.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
[2025-11-09T15:16:41.888+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 33)
[2025-11-09T15:16:41.889+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[76] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:41.904+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 465.9 KiB, free 433.3 MiB)
[2025-11-09T15:16:41.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 160.8 KiB, free 433.1 MiB)
[2025-11-09T15:16:41.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 863c17993851:44651 (size: 160.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:41.910+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:41.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[76] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:41.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0
[2025-11-09T15:16:41.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 92) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:41.914+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 93) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:41.929+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:41 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on 172.19.0.9:35743 (size: 160.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.029+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 92) in 116 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:42.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 93) in 129 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:42.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2025-11-09T15:16:42.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: ShuffleMapStage 33 (mapToPair at HoodieJavaRDD.java:149) finished in 0.153 s
[2025-11-09T15:16:42.044+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:42.044+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:42.044+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: waiting: Set(ResultStage 34)
[2025-11-09T15:16:42.044+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:42.045+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[81] at filter at HoodieSparkSqlWriter.scala:1050), which has no missing parents
[2025-11-09T15:16:42.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 483.6 KiB, free 432.7 MiB)
[2025-11-09T15:16:42.080+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 171.2 KiB, free 432.5 MiB)
[2025-11-09T15:16:42.084+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 863c17993851:44651 (size: 171.2 KiB, free: 433.9 MiB)
[2025-11-09T15:16:42.085+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:42.085+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 433.9 MiB)
[2025-11-09T15:16:42.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 34 (MapPartitionsRDD[81] at filter at HoodieSparkSqlWriter.scala:1050) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-11-09T15:16:42.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSchedulerImpl: Adding task set 34.0 with 31 tasks resource profile 0
[2025-11-09T15:16:42.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_21_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.089+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 94) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:42.089+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 95) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:42.090+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSetManager: Starting task 2.0 in stage 34.0 (TID 96) (172.19.0.9, executor 0, partition 2, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:42.090+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO TaskSetManager: Starting task 3.0 in stage 34.0 (TID 97) (172.19.0.9, executor 0, partition 3, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:42.094+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 863c17993851:44651 in memory (size: 159.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.110+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_22_piece0 on 172.19.0.9:35743 in memory (size: 159.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:42.120+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 863c17993851:44651 in memory (size: 12.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on 172.19.0.9:35743 (size: 171.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_19_piece0 on 172.19.0.9:35743 in memory (size: 12.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.129+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 863c17993851:44651 in memory (size: 5.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.133+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO BlockManagerInfo: Removed broadcast_20_piece0 on 172.19.0.9:35743 in memory (size: 5.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:42.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.19.0.9:56384
[2025-11-09T15:16:42.560+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MarkerHandler: Request: create marker: state=Georgia/66ef71e8-e89e-4103-8142-ac07db0f5d65-0_3-34-97_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:42.560+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MarkerHandler: Request: create marker: state=Rhode Island/8920d382-d722-49c9-9fe5-4e9af0f14ed2-0_2-34-96_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:42.561+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MarkerHandler: Request: create marker: state=West Virginia/330a489b-c47d-4238-843d-07cc7e1c184b-0_1-34-95_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:42.561+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:42 INFO MarkerHandler: Request: create marker: state=Indiana/3a0173bc-f484-488f-87a9-4910a3d954dd-0_0-34-94_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.330+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_2 in memory on 172.19.0.9:35743 (size: 332.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.332+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_1 in memory on 172.19.0.9:35743 (size: 334.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.333+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_3 in memory on 172.19.0.9:35743 (size: 327.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.338+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 4.0 in stage 34.0 (TID 98) (172.19.0.9, executor 0, partition 4, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.338+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 2.0 in stage 34.0 (TID 96) in 1249 ms on 172.19.0.9 (executor 0) (1/31)
[2025-11-09T15:16:43.339+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 5.0 in stage 34.0 (TID 99) (172.19.0.9, executor 0, partition 5, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.339+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 6.0 in stage 34.0 (TID 100) (172.19.0.9, executor 0, partition 6, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.343+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 95) in 1254 ms on 172.19.0.9 (executor 0) (2/31)
[2025-11-09T15:16:43.344+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 3.0 in stage 34.0 (TID 97) in 1253 ms on 172.19.0.9 (executor 0) (3/31)
[2025-11-09T15:16:43.369+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_0 in memory on 172.19.0.9:35743 (size: 327.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 7.0 in stage 34.0 (TID 101) (172.19.0.9, executor 0, partition 7, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.379+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 94) in 1291 ms on 172.19.0.9 (executor 0) (4/31)
[2025-11-09T15:16:43.485+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Tennessee/094034cc-584a-402b-a789-868b90101baf-0_6-34-100_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.486+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Michigan/0bb47ca3-8f2b-4a56-af51-21552e13edcd-0_5-34-99_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Colorado/53782343-b6b0-4fb2-9e8f-28ccc4e1dc0c-0_7-34-101_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Kansas/67686da3-45f5-4795-a2d7-c8f61db04304-0_4-34-98_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.618+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_7 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.624+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 8.0 in stage 34.0 (TID 102) (172.19.0.9, executor 0, partition 8, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.627+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 7.0 in stage 34.0 (TID 101) in 252 ms on 172.19.0.9 (executor 0) (5/31)
[2025-11-09T15:16:43.630+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_5 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.633+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_6 in memory on 172.19.0.9:35743 (size: 330.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.635+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 9.0 in stage 34.0 (TID 103) (172.19.0.9, executor 0, partition 9, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.638+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 5.0 in stage 34.0 (TID 99) in 300 ms on 172.19.0.9 (executor 0) (6/31)
[2025-11-09T15:16:43.639+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 10.0 in stage 34.0 (TID 104) (172.19.0.9, executor 0, partition 10, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.643+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 6.0 in stage 34.0 (TID 100) in 303 ms on 172.19.0.9 (executor 0) (7/31)
[2025-11-09T15:16:43.670+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_4 in memory on 172.19.0.9:35743 (size: 325.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.675+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 11.0 in stage 34.0 (TID 105) (172.19.0.9, executor 0, partition 11, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.679+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 4.0 in stage 34.0 (TID 98) in 341 ms on 172.19.0.9 (executor 0) (8/31)
[2025-11-09T15:16:43.779+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Mississippi/091e9fbf-9981-408e-ae56-23e2ca37c9b2-0_8-34-102_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.783+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Washington/5f0a60e9-b3e1-4335-ab4c-8cbcdd12a42c-0_10-34-104_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.789+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=Wyoming/308bf108-59dd-4771-b5fa-9d1326c06e34-0_9-34-103_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.832+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=North Carolina/fd59f0d7-2eca-408e-8690-ce480c44ee24-0_11-34-105_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:43.865+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_8 in memory on 172.19.0.9:35743 (size: 330.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.871+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 12.0 in stage 34.0 (TID 106) (172.19.0.9, executor 0, partition 12, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.875+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 8.0 in stage 34.0 (TID 102) in 251 ms on 172.19.0.9 (executor 0) (9/31)
[2025-11-09T15:16:43.915+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_11 in memory on 172.19.0.9:35743 (size: 337.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.915+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_9 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.917+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO BlockManagerInfo: Added rdd_80_10 in memory on 172.19.0.9:35743 (size: 332.0 B, free: 434.1 MiB)
[2025-11-09T15:16:43.922+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 13.0 in stage 34.0 (TID 107) (172.19.0.9, executor 0, partition 13, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.923+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 14.0 in stage 34.0 (TID 108) (172.19.0.9, executor 0, partition 14, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.924+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Starting task 15.0 in stage 34.0 (TID 109) (172.19.0.9, executor 0, partition 15, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:43.925+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 11.0 in stage 34.0 (TID 105) in 250 ms on 172.19.0.9 (executor 0) (10/31)
[2025-11-09T15:16:43.927+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 9.0 in stage 34.0 (TID 103) in 292 ms on 172.19.0.9 (executor 0) (11/31)
[2025-11-09T15:16:43.927+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO TaskSetManager: Finished task 10.0 in stage 34.0 (TID 104) in 288 ms on 172.19.0.9 (executor 0) (12/31)
[2025-11-09T15:16:43.995+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:43 INFO MarkerHandler: Request: create marker: state=North Dakota/fbd2796a-f32c-45b8-b115-f48bb5385efd-0_12-34-106_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.030+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=South Dakota/d8f894b4-3b27-430b-8d0c-5e92c322475b-0_13-34-107_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.062+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Oklahoma/acbb0254-8ce1-4d86-9cfc-b17b69c0df19-0_15-34-109_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.081+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Idaho/3f3ee679-9efc-4bf8-8174-575fdf54d652-0_14-34-108_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.110+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_12 in memory on 172.19.0.9:35743 (size: 334.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.114+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_13 in memory on 172.19.0.9:35743 (size: 334.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.115+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 16.0 in stage 34.0 (TID 110) (172.19.0.9, executor 0, partition 16, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 17.0 in stage 34.0 (TID 111) (172.19.0.9, executor 0, partition 17, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 12.0 in stage 34.0 (TID 106) in 251 ms on 172.19.0.9 (executor 0) (13/31)
[2025-11-09T15:16:44.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 13.0 in stage 34.0 (TID 107) in 204 ms on 172.19.0.9 (executor 0) (14/31)
[2025-11-09T15:16:44.169+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_15 in memory on 172.19.0.9:35743 (size: 330.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.170+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_14 in memory on 172.19.0.9:35743 (size: 327.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.175+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 18.0 in stage 34.0 (TID 112) (172.19.0.9, executor 0, partition 18, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.176+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 19.0 in stage 34.0 (TID 113) (172.19.0.9, executor 0, partition 19, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.178+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 15.0 in stage 34.0 (TID 109) in 253 ms on 172.19.0.9 (executor 0) (15/31)
[2025-11-09T15:16:44.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 14.0 in stage 34.0 (TID 108) in 256 ms on 172.19.0.9 (executor 0) (16/31)
[2025-11-09T15:16:44.239+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Nevada/a7a6e145-8756-40a4-a03d-b8a73ced5716-0_16-34-110_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.241+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Alaska/466519aa-c80d-41a8-82aa-e1b2f0e5b5a5-0_17-34-111_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.301+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Kentucky/793f9a66-8c63-4649-ac94-46e25694b8f0-0_18-34-112_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.301+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Montana/6b8f87a1-b82f-441c-aa10-1213cb8ce91b-0_19-34-113_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.368+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_16 in memory on 172.19.0.9:35743 (size: 327.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.369+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_17 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 20.0 in stage 34.0 (TID 114) (172.19.0.9, executor 0, partition 20, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 21.0 in stage 34.0 (TID 115) (172.19.0.9, executor 0, partition 21, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.379+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 17.0 in stage 34.0 (TID 111) in 258 ms on 172.19.0.9 (executor 0) (17/31)
[2025-11-09T15:16:44.379+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 16.0 in stage 34.0 (TID 110) in 265 ms on 172.19.0.9 (executor 0) (18/31)
[2025-11-09T15:16:44.422+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_18 in memory on 172.19.0.9:35743 (size: 330.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.422+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_19 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.426+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 22.0 in stage 34.0 (TID 116) (172.19.0.9, executor 0, partition 22, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.427+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 23.0 in stage 34.0 (TID 117) (172.19.0.9, executor 0, partition 23, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.430+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 19.0 in stage 34.0 (TID 113) in 254 ms on 172.19.0.9 (executor 0) (19/31)
[2025-11-09T15:16:44.430+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 18.0 in stage 34.0 (TID 112) in 256 ms on 172.19.0.9 (executor 0) (20/31)
[2025-11-09T15:16:44.492+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Virginia/4aa8d485-d66f-4883-b3c9-23e1b8fbda88-0_20-34-114_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.493+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Iowa/3014e5b2-69e4-4275-bccb-573e691efc52-0_21-34-115_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.543+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Minnesota/1dd3e763-b4f8-429a-898c-5e00bea6ccb0-0_22-34-116_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.544+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Illinois/51dff66d-e2a6-4948-9ef4-71d55bd07aa9-0_23-34-117_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.617+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_21 in memory on 172.19.0.9:35743 (size: 325.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.623+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 24.0 in stage 34.0 (TID 118) (172.19.0.9, executor 0, partition 24, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.626+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 21.0 in stage 34.0 (TID 115) in 250 ms on 172.19.0.9 (executor 0) (21/31)
[2025-11-09T15:16:44.627+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_20 in memory on 172.19.0.9:35743 (size: 330.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.635+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 25.0 in stage 34.0 (TID 119) (172.19.0.9, executor 0, partition 25, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.638+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 20.0 in stage 34.0 (TID 114) in 264 ms on 172.19.0.9 (executor 0) (22/31)
[2025-11-09T15:16:44.666+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_23 in memory on 172.19.0.9:35743 (size: 329.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.666+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_22 in memory on 172.19.0.9:35743 (size: 331.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.671+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 26.0 in stage 34.0 (TID 120) (172.19.0.9, executor 0, partition 26, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.672+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 27.0 in stage 34.0 (TID 121) (172.19.0.9, executor 0, partition 27, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.674+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 22.0 in stage 34.0 (TID 116) in 248 ms on 172.19.0.9 (executor 0) (23/31)
[2025-11-09T15:16:44.676+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 23.0 in stage 34.0 (TID 117) in 248 ms on 172.19.0.9 (executor 0) (24/31)
[2025-11-09T15:16:44.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Nebraska/8ec9efbc-37c6-4c1c-afd8-657ff46a3d10-0_24-34-118_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.747+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Arkansas/fdae6120-1db3-4983-b21f-f1c07e527548-0_25-34-119_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.784+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Maine/e6253b0e-8776-4219-9f07-4c166df69fd3-0_27-34-121_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.785+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Hawaii/5e2930c8-8d27-46c7-9319-21139015f15b-0_26-34-120_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.857+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_25 in memory on 172.19.0.9:35743 (size: 330.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.858+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_24 in memory on 172.19.0.9:35743 (size: 329.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.863+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 28.0 in stage 34.0 (TID 122) (172.19.0.9, executor 0, partition 28, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.864+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 29.0 in stage 34.0 (TID 123) (172.19.0.9, executor 0, partition 29, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.866+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 25.0 in stage 34.0 (TID 119) in 232 ms on 172.19.0.9 (executor 0) (25/31)
[2025-11-09T15:16:44.867+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 24.0 in stage 34.0 (TID 118) in 244 ms on 172.19.0.9 (executor 0) (26/31)
[2025-11-09T15:16:44.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_27 in memory on 172.19.0.9:35743 (size: 326.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.916+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO BlockManagerInfo: Added rdd_80_26 in memory on 172.19.0.9:35743 (size: 327.0 B, free: 434.1 MiB)
[2025-11-09T15:16:44.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Starting task 30.0 in stage 34.0 (TID 124) (172.19.0.9, executor 0, partition 30, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:44.923+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 27.0 in stage 34.0 (TID 121) in 250 ms on 172.19.0.9 (executor 0) (27/31)
[2025-11-09T15:16:44.925+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO TaskSetManager: Finished task 26.0 in stage 34.0 (TID 120) in 254 ms on 172.19.0.9 (executor 0) (28/31)
[2025-11-09T15:16:44.970+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Florida/8354dfe4-0f4e-4f7b-a7ab-7ddd10de2070-0_28-34-122_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:44.970+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:44 INFO MarkerHandler: Request: create marker: state=Wisconsin/2a0b55c8-c99e-43b7-b14a-21f8ef275fca-0_29-34-123_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:45.024+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO MarkerHandler: Request: create marker: state=Arizona/80a08e51-591e-4a3f-8a95-532af35b52c5-0_30-34-124_20251109151634225.parquet.marker.CREATE
[2025-11-09T15:16:45.062+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added rdd_80_28 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:45.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 28.0 in stage 34.0 (TID 122) in 208 ms on 172.19.0.9 (executor 0) (29/31)
[2025-11-09T15:16:45.106+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added rdd_80_29 in memory on 172.19.0.9:35743 (size: 331.0 B, free: 434.1 MiB)
[2025-11-09T15:16:45.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 29.0 in stage 34.0 (TID 123) in 249 ms on 172.19.0.9 (executor 0) (30/31)
[2025-11-09T15:16:45.116+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added rdd_80_30 in memory on 172.19.0.9:35743 (size: 328.0 B, free: 434.1 MiB)
[2025-11-09T15:16:45.123+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 30.0 in stage 34.0 (TID 124) in 204 ms on 172.19.0.9 (executor 0) (31/31)
[2025-11-09T15:16:45.124+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2025-11-09T15:16:45.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: ResultStage 34 (count at HoodieSparkSqlWriter.scala:1050) finished in 3.077 s
[2025-11-09T15:16:45.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:45.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2025-11-09T15:16:45.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Job 17 finished: count at HoodieSparkSqlWriter.scala:1050, took 3.241555 s
[2025-11-09T15:16:45.126+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieSparkSqlWriter$: Proceeding to commit the write.
[2025-11-09T15:16:45.157+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
[2025-11-09T15:16:45.160+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Got job 18 (collect at SparkRDDWriteClient.java:103) with 31 output partitions
[2025-11-09T15:16:45.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Final stage: ResultStage 40 (collect at SparkRDDWriteClient.java:103)
[2025-11-09T15:16:45.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)
[2025-11-09T15:16:45.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:45.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[82] at map at SparkRDDWriteClient.java:103), which has no missing parents
[2025-11-09T15:16:45.175+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 484.3 KiB, free 432.7 MiB)
[2025-11-09T15:16:45.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 171.6 KiB, free 432.5 MiB)
[2025-11-09T15:16:45.180+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 863c17993851:44651 (size: 171.6 KiB, free: 433.9 MiB)
[2025-11-09T15:16:45.181+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:45.181+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 40 (MapPartitionsRDD[82] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-11-09T15:16:45.182+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSchedulerImpl: Adding task set 40.0 with 31 tasks resource profile 0
[2025-11-09T15:16:45.182+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 125) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.183+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 126) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.183+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 2.0 in stage 40.0 (TID 127) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.183+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 3.0 in stage 40.0 (TID 128) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.197+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on 172.19.0.9:35743 (size: 171.6 KiB, free: 433.9 MiB)
[2025-11-09T15:16:45.229+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 4.0 in stage 40.0 (TID 129) (172.19.0.9, executor 0, partition 4, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 5.0 in stage 40.0 (TID 130) (172.19.0.9, executor 0, partition 5, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.231+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 3.0 in stage 40.0 (TID 128) in 47 ms on 172.19.0.9 (executor 0) (1/31)
[2025-11-09T15:16:45.232+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 6.0 in stage 40.0 (TID 131) (172.19.0.9, executor 0, partition 6, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.233+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 7.0 in stage 40.0 (TID 132) (172.19.0.9, executor 0, partition 7, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.233+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 125) in 51 ms on 172.19.0.9 (executor 0) (2/31)
[2025-11-09T15:16:45.235+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 126) in 53 ms on 172.19.0.9 (executor 0) (3/31)
[2025-11-09T15:16:45.236+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 2.0 in stage 40.0 (TID 127) in 53 ms on 172.19.0.9 (executor 0) (4/31)
[2025-11-09T15:16:45.256+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 8.0 in stage 40.0 (TID 133) (172.19.0.9, executor 0, partition 8, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.257+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 9.0 in stage 40.0 (TID 134) (172.19.0.9, executor 0, partition 9, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.258+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 10.0 in stage 40.0 (TID 135) (172.19.0.9, executor 0, partition 10, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.259+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 5.0 in stage 40.0 (TID 130) in 29 ms on 172.19.0.9 (executor 0) (5/31)
[2025-11-09T15:16:45.260+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 11.0 in stage 40.0 (TID 136) (172.19.0.9, executor 0, partition 11, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.260+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 6.0 in stage 40.0 (TID 131) in 29 ms on 172.19.0.9 (executor 0) (6/31)
[2025-11-09T15:16:45.261+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 7.0 in stage 40.0 (TID 132) in 29 ms on 172.19.0.9 (executor 0) (7/31)
[2025-11-09T15:16:45.262+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 4.0 in stage 40.0 (TID 129) in 33 ms on 172.19.0.9 (executor 0) (8/31)
[2025-11-09T15:16:45.276+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 12.0 in stage 40.0 (TID 137) (172.19.0.9, executor 0, partition 12, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.277+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 13.0 in stage 40.0 (TID 138) (172.19.0.9, executor 0, partition 13, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.277+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 14.0 in stage 40.0 (TID 139) (172.19.0.9, executor 0, partition 14, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.278+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 15.0 in stage 40.0 (TID 140) (172.19.0.9, executor 0, partition 15, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.278+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 9.0 in stage 40.0 (TID 134) in 21 ms on 172.19.0.9 (executor 0) (9/31)
[2025-11-09T15:16:45.279+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 11.0 in stage 40.0 (TID 136) in 20 ms on 172.19.0.9 (executor 0) (10/31)
[2025-11-09T15:16:45.280+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 10.0 in stage 40.0 (TID 135) in 21 ms on 172.19.0.9 (executor 0) (11/31)
[2025-11-09T15:16:45.280+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 8.0 in stage 40.0 (TID 133) in 25 ms on 172.19.0.9 (executor 0) (12/31)
[2025-11-09T15:16:45.296+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 16.0 in stage 40.0 (TID 141) (172.19.0.9, executor 0, partition 16, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 17.0 in stage 40.0 (TID 142) (172.19.0.9, executor 0, partition 17, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 18.0 in stage 40.0 (TID 143) (172.19.0.9, executor 0, partition 18, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 19.0 in stage 40.0 (TID 144) (172.19.0.9, executor 0, partition 19, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 14.0 in stage 40.0 (TID 139) in 21 ms on 172.19.0.9 (executor 0) (13/31)
[2025-11-09T15:16:45.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 13.0 in stage 40.0 (TID 138) in 23 ms on 172.19.0.9 (executor 0) (14/31)
[2025-11-09T15:16:45.300+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 15.0 in stage 40.0 (TID 140) in 22 ms on 172.19.0.9 (executor 0) (15/31)
[2025-11-09T15:16:45.301+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 12.0 in stage 40.0 (TID 137) in 25 ms on 172.19.0.9 (executor 0) (16/31)
[2025-11-09T15:16:45.313+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 20.0 in stage 40.0 (TID 145) (172.19.0.9, executor 0, partition 20, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.314+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 21.0 in stage 40.0 (TID 146) (172.19.0.9, executor 0, partition 21, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.315+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 22.0 in stage 40.0 (TID 147) (172.19.0.9, executor 0, partition 22, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.316+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 23.0 in stage 40.0 (TID 148) (172.19.0.9, executor 0, partition 23, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.316+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 18.0 in stage 40.0 (TID 143) in 19 ms on 172.19.0.9 (executor 0) (17/31)
[2025-11-09T15:16:45.317+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 19.0 in stage 40.0 (TID 144) in 20 ms on 172.19.0.9 (executor 0) (18/31)
[2025-11-09T15:16:45.318+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 17.0 in stage 40.0 (TID 142) in 22 ms on 172.19.0.9 (executor 0) (19/31)
[2025-11-09T15:16:45.319+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 16.0 in stage 40.0 (TID 141) in 23 ms on 172.19.0.9 (executor 0) (20/31)
[2025-11-09T15:16:45.339+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 24.0 in stage 40.0 (TID 149) (172.19.0.9, executor 0, partition 24, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.340+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 25.0 in stage 40.0 (TID 150) (172.19.0.9, executor 0, partition 25, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.341+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 26.0 in stage 40.0 (TID 151) (172.19.0.9, executor 0, partition 26, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.341+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 27.0 in stage 40.0 (TID 152) (172.19.0.9, executor 0, partition 27, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.342+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 22.0 in stage 40.0 (TID 147) in 27 ms on 172.19.0.9 (executor 0) (21/31)
[2025-11-09T15:16:45.342+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 20.0 in stage 40.0 (TID 145) in 29 ms on 172.19.0.9 (executor 0) (22/31)
[2025-11-09T15:16:45.342+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 23.0 in stage 40.0 (TID 148) in 27 ms on 172.19.0.9 (executor 0) (23/31)
[2025-11-09T15:16:45.344+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 21.0 in stage 40.0 (TID 146) in 30 ms on 172.19.0.9 (executor 0) (24/31)
[2025-11-09T15:16:45.360+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 28.0 in stage 40.0 (TID 153) (172.19.0.9, executor 0, partition 28, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.361+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 29.0 in stage 40.0 (TID 154) (172.19.0.9, executor 0, partition 29, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.361+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 30.0 in stage 40.0 (TID 155) (172.19.0.9, executor 0, partition 30, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:45.363+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 27.0 in stage 40.0 (TID 152) in 22 ms on 172.19.0.9 (executor 0) (25/31)
[2025-11-09T15:16:45.363+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 24.0 in stage 40.0 (TID 149) in 25 ms on 172.19.0.9 (executor 0) (26/31)
[2025-11-09T15:16:45.364+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 26.0 in stage 40.0 (TID 151) in 23 ms on 172.19.0.9 (executor 0) (27/31)
[2025-11-09T15:16:45.364+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 25.0 in stage 40.0 (TID 150) in 24 ms on 172.19.0.9 (executor 0) (28/31)
[2025-11-09T15:16:45.377+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 28.0 in stage 40.0 (TID 153) in 16 ms on 172.19.0.9 (executor 0) (29/31)
[2025-11-09T15:16:45.377+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 30.0 in stage 40.0 (TID 155) in 16 ms on 172.19.0.9 (executor 0) (30/31)
[2025-11-09T15:16:45.377+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Finished task 29.0 in stage 40.0 (TID 154) in 16 ms on 172.19.0.9 (executor 0) (31/31)
[2025-11-09T15:16:45.378+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-11-09T15:16:45.379+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: ResultStage 40 (collect at SparkRDDWriteClient.java:103) finished in 0.216 s
[2025-11-09T15:16:45.380+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:45.380+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2025-11-09T15:16:45.380+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Job 18 finished: collect at SparkRDDWriteClient.java:103, took 0.222262 s
[2025-11-09T15:16:45.380+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BaseHoodieWriteClient: Committing 20251109151634225 action commit
[2025-11-09T15:16:45.381+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.389+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:45.398+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.398+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.405+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__INFLIGHT__20251109151641829]}
[2025-11-09T15:16:45.406+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.413+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:45.421+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.421+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:45.427+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.434+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:45.440+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.440+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:45.440+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:45.440+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:45.440+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:45.440+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:31 numReplaceFileIds:0
[2025-11-09T15:16:45.442+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.449+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:45.456+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.457+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.461+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__commit__INFLIGHT__20251109151641829]}
[2025-11-09T15:16:45.461+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.468+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:45.474+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.474+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:45.478+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.483+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:45.489+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.490+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:45.490+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:45.490+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:45.490+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:45.490+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BaseHoodieWriteClient: Committing 20251109151634225 action commit
[2025-11-09T15:16:45.567+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 863c17993851:44651 in memory (size: 171.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:45.569+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Removed broadcast_24_piece0 on 172.19.0.9:35743 in memory (size: 171.2 KiB, free: 434.1 MiB)
[2025-11-09T15:16:45.577+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 863c17993851:44651 in memory (size: 160.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:45.578+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Removed broadcast_23_piece0 on 172.19.0.9:35743 in memory (size: 160.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:45.586+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 863c17993851:44651 in memory (size: 171.6 KiB, free: 434.4 MiB)
[2025-11-09T15:16:45.586+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Removed broadcast_25_piece0 on 172.19.0.9:35743 in memory (size: 171.6 KiB, free: 434.4 MiB)
[2025-11-09T15:16:45.668+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.675+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:45.683+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.687+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.694+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.700+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.707+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
[2025-11-09T15:16:45.707+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.714+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:45.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:45.722+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:45.729+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.735+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:45.741+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:45.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:45.750+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetadataUtil: Updating at 20251109151634225 from Commit/UPSERT. #partitions_updated=32, #files_added=31
[2025-11-09T15:16:45.776+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:45.776+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:45.776+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-11-09T15:16:45.777+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:45.777+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:45.777+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-11-09T15:16:45.786+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:45.786+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:45.786+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.793+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.800+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.800+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.805+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.810+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.817+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:45.817+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:45.817+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieBackedTableMetadataWriter: New commit at 20251109151634225 being applied to MDT.
[2025-11-09T15:16:45.817+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.825+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.833+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.833+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.838+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.838+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO CleanerUtils: Cleaned failed attempts if any
[2025-11-09T15:16:45.838+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.844+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.857+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151639098]}
[2025-11-09T15:16:45.863+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.871+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:45.872+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:45.872+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BaseHoodieWriteClient: Generate a new instant time: 20251109151634225 action: deltacommit
[2025-11-09T15:16:45.872+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Creating a new instant [==>20251109151634225__deltacommit__REQUESTED]
[2025-11-09T15:16:45.894+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:45.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151634225__deltacommit__REQUESTED__20251109151645882]}
[2025-11-09T15:16:45.916+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:45.924+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:45.925+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:45.925+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-11-09T15:16:45.925+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-11-09T15:16:45.927+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:45.927+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:45.936+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-11-09T15:16:45.937+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Registering RDD 93 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 10
[2025-11-09T15:16:45.938+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Got job 19 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
[2025-11-09T15:16:45.938+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Final stage: ResultStage 42 (countByKey at HoodieJavaPairRDD.java:105)
[2025-11-09T15:16:45.938+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
[2025-11-09T15:16:45.939+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
[2025-11-09T15:16:45.940+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[93] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:45.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 10.1 KiB, free 434.4 MiB)
[2025-11-09T15:16:45.946+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 434.4 MiB)
[2025-11-09T15:16:45.946+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 863c17993851:44651 (size: 5.5 KiB, free: 434.4 MiB)
[2025-11-09T15:16:45.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:45.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[93] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:45.948+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2025-11-09T15:16:45.952+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 156) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 12092 bytes)
[2025-11-09T15:16:45.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on 172.19.0.9:35743 (size: 5.5 KiB, free: 434.4 MiB)
[2025-11-09T15:16:45.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:45 INFO BlockManagerInfo: Added rdd_91_0 in memory on 172.19.0.9:35743 (size: 2.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:46.001+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 156) in 52 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:46.002+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-11-09T15:16:46.002+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: ShuffleMapStage 41 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.061 s
[2025-11-09T15:16:46.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:46.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:46.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: waiting: Set(ResultStage 42)
[2025-11-09T15:16:46.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:46.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting ResultStage 42 (ShuffledRDD[94] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:46.006+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.5 KiB, free 434.4 MiB)
[2025-11-09T15:16:46.011+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.4 MiB)
[2025-11-09T15:16:46.012+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:46.013+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:46.013+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (ShuffledRDD[94] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:46.014+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
[2025-11-09T15:16:46.016+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 157) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:46.030+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:46.036+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.19.0.9:56384
[2025-11-09T15:16:46.051+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 157) in 35 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:46.051+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool
[2025-11-09T15:16:46.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: ResultStage 42 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.048 s
[2025-11-09T15:16:46.053+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:46.053+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
[2025-11-09T15:16:46.054+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Job 19 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.117178 s
[2025-11-09T15:16:46.055+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-11-09T15:16:46.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-11-09T15:16:46.096+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Got job 20 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
[2025-11-09T15:16:46.096+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Final stage: ResultStage 43 (collectAsMap at UpsertPartitioner.java:282)
[2025-11-09T15:16:46.097+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:46.097+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:46.097+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[96] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-11-09T15:16:46.108+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 352.5 KiB, free 434.0 MiB)
[2025-11-09T15:16:46.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 123.6 KiB, free 433.9 MiB)
[2025-11-09T15:16:46.112+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 863c17993851:44651 (size: 123.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:46.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:46.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[96] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:46.114+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-11-09T15:16:46.114+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 158) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7253 bytes)
[2025-11-09T15:16:46.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on 172.19.0.9:35743 (size: 123.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:46.177+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 158) in 63 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:46.178+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-11-09T15:16:46.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: ResultStage 43 (collectAsMap at UpsertPartitioner.java:282) finished in 0.082 s
[2025-11-09T15:16:46.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:46.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2025-11-09T15:16:46.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Job 20 finished: collectAsMap at UpsertPartitioner.java:282, took 0.083782 s
[2025-11-09T15:16:46.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:46.180+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:46.180+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO UpsertPartitioner: Total Buckets: 1
[2025-11-09T15:16:46.181+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251109151634225.deltacommit.requested
[2025-11-09T15:16:46.208+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251109151634225.deltacommit.inflight
[2025-11-09T15:16:46.226+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-11-09T15:16:46.226+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20251109151634225
[2025-11-09T15:16:46.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-11-09T15:16:46.253+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Registering RDD 97 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 11
[2025-11-09T15:16:46.253+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Got job 21 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-11-09T15:16:46.253+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Final stage: ResultStage 45 (collect at HoodieJavaRDD.java:177)
[2025-11-09T15:16:46.253+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
[2025-11-09T15:16:46.254+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 44)
[2025-11-09T15:16:46.254+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[97] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:46.263+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 357.3 KiB, free 433.6 MiB)
[2025-11-09T15:16:46.267+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 123.9 KiB, free 433.4 MiB)
[2025-11-09T15:16:46.267+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 863c17993851:44651 (size: 123.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:46.268+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:46.269+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[97] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:46.269+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
[2025-11-09T15:16:46.272+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 159) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 12092 bytes)
[2025-11-09T15:16:46.280+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on 172.19.0.9:35743 (size: 123.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:46.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 159) in 27 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:46.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool
[2025-11-09T15:16:46.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: ShuffleMapStage 44 (mapToPair at HoodieJavaRDD.java:149) finished in 0.042 s
[2025-11-09T15:16:46.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:46.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:46.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: waiting: Set(ResultStage 45)
[2025-11-09T15:16:46.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:46.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[102] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-11-09T15:16:46.310+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 459.4 KiB, free 433.0 MiB)
[2025-11-09T15:16:46.322+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 162.5 KiB, free 432.8 MiB)
[2025-11-09T15:16:46.325+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 863c17993851:44651 in memory (size: 123.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:46.325+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Removed broadcast_28_piece0 on 172.19.0.9:35743 in memory (size: 123.6 KiB, free: 434.3 MiB)
[2025-11-09T15:16:46.326+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 863c17993851:44651 (size: 162.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:46.326+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:46.327+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[102] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:46.327+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
[2025-11-09T15:16:46.328+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 160) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:46.330+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:46.331+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Removed broadcast_27_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.3 MiB)
[2025-11-09T15:16:46.348+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 863c17993851:44651 in memory (size: 5.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:46.348+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Removed broadcast_26_piece0 on 172.19.0.9:35743 in memory (size: 5.5 KiB, free: 434.3 MiB)
[2025-11-09T15:16:46.353+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on 172.19.0.9:35743 (size: 162.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:46.371+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:46 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.19.0.9:56384
[2025-11-09T15:16:47.334+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added rdd_101_0 in memory on 172.19.0.9:35743 (size: 367.0 B, free: 434.1 MiB)
[2025-11-09T15:16:47.344+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 160) in 1016 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:47.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2025-11-09T15:16:47.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: ResultStage 45 (collect at HoodieJavaRDD.java:177) finished in 1.047 s
[2025-11-09T15:16:47.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:47.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
[2025-11-09T15:16:47.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 21 finished: collect at HoodieJavaRDD.java:177, took 1.094037 s
[2025-11-09T15:16:47.347+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
[2025-11-09T15:16:47.347+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseSparkCommitActionExecutor: Committing 20251109151634225, action Type deltacommit, operation Type UPSERT_PREPPED
[2025-11-09T15:16:47.374+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-11-09T15:16:47.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Got job 22 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-11-09T15:16:47.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Final stage: ResultStage 46 (collect at HoodieSparkEngineContext.java:150)
[2025-11-09T15:16:47.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:47.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:47.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[104] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-11-09T15:16:47.379+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 100.7 KiB, free 433.2 MiB)
[2025-11-09T15:16:47.383+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.2 MiB)
[2025-11-09T15:16:47.383+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 863c17993851:44651 (size: 35.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:47.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:47.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[104] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:47.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2025-11-09T15:16:47.386+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 161) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-11-09T15:16:47.396+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on 172.19.0.9:35743 (size: 35.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:47.408+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 161) in 23 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:47.409+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2025-11-09T15:16:47.409+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: ResultStage 46 (collect at HoodieSparkEngineContext.java:150) finished in 0.033 s
[2025-11-09T15:16:47.409+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:47.409+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2025-11-09T15:16:47.409+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 22 finished: collect at HoodieSparkEngineContext.java:150, took 0.035447 s
[2025-11-09T15:16:47.417+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Marking instant complete [==>20251109151634225__deltacommit__INFLIGHT]
[2025-11-09T15:16:47.418+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251109151634225.deltacommit.inflight
[2025-11-09T15:16:47.441+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251109151634225.deltacommit
[2025-11-09T15:16:47.441+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Completed [==>20251109151634225__deltacommit__INFLIGHT]
[2025-11-09T15:16:47.442+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseSparkCommitActionExecutor: Committed 20251109151634225
[2025-11-09T15:16:47.466+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-11-09T15:16:47.467+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Got job 23 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-11-09T15:16:47.468+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Final stage: ResultStage 47 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-11-09T15:16:47.468+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:47.468+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:47.468+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[106] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-11-09T15:16:47.472+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 100.9 KiB, free 433.1 MiB)
[2025-11-09T15:16:47.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.1 MiB)
[2025-11-09T15:16:47.476+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 863c17993851:44651 (size: 36.0 KiB, free: 434.1 MiB)
[2025-11-09T15:16:47.476+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:47.477+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[106] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:47.477+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0
[2025-11-09T15:16:47.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 162) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-11-09T15:16:47.493+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on 172.19.0.9:35743 (size: 36.0 KiB, free: 434.0 MiB)
[2025-11-09T15:16:47.538+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 162) in 60 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:47.539+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool
[2025-11-09T15:16:47.539+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: ResultStage 47 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.070 s
[2025-11-09T15:16:47.539+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:47.540+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
[2025-11-09T15:16:47.540+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 23 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.073460 s
[2025-11-09T15:16:47.571+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/.temp/20251109151634225
[2025-11-09T15:16:47.571+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:47.580+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:47.589+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:47.589+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-11-09T15:16:47.594+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__deltacommit__COMPLETED__20251109151647428]}
[2025-11-09T15:16:47.603+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:47.610+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:47.611+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:47.616+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__deltacommit__COMPLETED__20251109151647428]}
[2025-11-09T15:16:47.617+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Marking instant complete [==>20251109151634225__commit__INFLIGHT]
[2025-11-09T15:16:47.617+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/20251109151634225.inflight
[2025-11-09T15:16:47.642+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/20251109151634225.commit
[2025-11-09T15:16:47.642+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Completed [==>20251109151634225__commit__INFLIGHT]
[2025-11-09T15:16:47.672+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-11-09T15:16:47.674+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Got job 24 (collectAsMap at HoodieSparkEngineContext.java:164) with 11 output partitions
[2025-11-09T15:16:47.674+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Final stage: ResultStage 48 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-11-09T15:16:47.674+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:47.674+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:47.674+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[108] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-11-09T15:16:47.678+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 100.9 KiB, free 433.0 MiB)
[2025-11-09T15:16:47.680+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 432.9 MiB)
[2025-11-09T15:16:47.681+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 863c17993851:44651 (size: 36.0 KiB, free: 434.0 MiB)
[2025-11-09T15:16:47.681+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:47.682+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Submitting 11 missing tasks from ResultStage 48 (MapPartitionsRDD[108] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10))
[2025-11-09T15:16:47.682+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Adding task set 48.0 with 11 tasks resource profile 0
[2025-11-09T15:16:47.683+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 163) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7337 bytes)
[2025-11-09T15:16:47.683+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 164) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7333 bytes)
[2025-11-09T15:16:47.683+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 2.0 in stage 48.0 (TID 165) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7334 bytes)
[2025-11-09T15:16:47.683+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 3.0 in stage 48.0 (TID 166) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7334 bytes)
[2025-11-09T15:16:47.695+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on 172.19.0.9:35743 (size: 36.0 KiB, free: 434.0 MiB)
[2025-11-09T15:16:47.711+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 4.0 in stage 48.0 (TID 167) (172.19.0.9, executor 0, partition 4, PROCESS_LOCAL, 7334 bytes)
[2025-11-09T15:16:47.713+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 5.0 in stage 48.0 (TID 168) (172.19.0.9, executor 0, partition 5, PROCESS_LOCAL, 7334 bytes)
[2025-11-09T15:16:47.714+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 164) in 30 ms on 172.19.0.9 (executor 0) (1/11)
[2025-11-09T15:16:47.714+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 6.0 in stage 48.0 (TID 169) (172.19.0.9, executor 0, partition 6, PROCESS_LOCAL, 7333 bytes)
[2025-11-09T15:16:47.717+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 7.0 in stage 48.0 (TID 170) (172.19.0.9, executor 0, partition 7, PROCESS_LOCAL, 7333 bytes)
[2025-11-09T15:16:47.717+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 3.0 in stage 48.0 (TID 166) in 34 ms on 172.19.0.9 (executor 0) (2/11)
[2025-11-09T15:16:47.717+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 163) in 35 ms on 172.19.0.9 (executor 0) (3/11)
[2025-11-09T15:16:47.718+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 2.0 in stage 48.0 (TID 165) in 34 ms on 172.19.0.9 (executor 0) (4/11)
[2025-11-09T15:16:47.730+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 8.0 in stage 48.0 (TID 171) (172.19.0.9, executor 0, partition 8, PROCESS_LOCAL, 7333 bytes)
[2025-11-09T15:16:47.731+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 4.0 in stage 48.0 (TID 167) in 20 ms on 172.19.0.9 (executor 0) (5/11)
[2025-11-09T15:16:47.738+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 9.0 in stage 48.0 (TID 172) (172.19.0.9, executor 0, partition 9, PROCESS_LOCAL, 7333 bytes)
[2025-11-09T15:16:47.741+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Starting task 10.0 in stage 48.0 (TID 173) (172.19.0.9, executor 0, partition 10, PROCESS_LOCAL, 7333 bytes)
[2025-11-09T15:16:47.741+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 5.0 in stage 48.0 (TID 168) in 30 ms on 172.19.0.9 (executor 0) (6/11)
[2025-11-09T15:16:47.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 6.0 in stage 48.0 (TID 169) in 28 ms on 172.19.0.9 (executor 0) (7/11)
[2025-11-09T15:16:47.745+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 7.0 in stage 48.0 (TID 170) in 30 ms on 172.19.0.9 (executor 0) (8/11)
[2025-11-09T15:16:47.754+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 8.0 in stage 48.0 (TID 171) in 27 ms on 172.19.0.9 (executor 0) (9/11)
[2025-11-09T15:16:47.770+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 9.0 in stage 48.0 (TID 172) in 33 ms on 172.19.0.9 (executor 0) (10/11)
[2025-11-09T15:16:47.783+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSetManager: Finished task 10.0 in stage 48.0 (TID 173) in 44 ms on 172.19.0.9 (executor 0) (11/11)
[2025-11-09T15:16:47.784+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2025-11-09T15:16:47.784+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: ResultStage 48 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.109 s
[2025-11-09T15:16:47.784+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:47.785+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2025-11-09T15:16:47.785+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO DAGScheduler: Job 24 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.112377 s
[2025-11-09T15:16:47.817+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=customers/.hoodie/.temp/20251109151634225
[2025-11-09T15:16:47.819+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseHoodieWriteClient: Committed 20251109151634225
[2025-11-09T15:16:47.825+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MapPartitionsRDD: Removing RDD 70 from persistence list
[2025-11-09T15:16:47.826+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManager: Removing RDD 70
[2025-11-09T15:16:47.827+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MapPartitionsRDD: Removing RDD 80 from persistence list
[2025-11-09T15:16:47.828+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManager: Removing RDD 80
[2025-11-09T15:16:47.828+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO UnionRDD: Removing RDD 91 from persistence list
[2025-11-09T15:16:47.829+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManager: Removing RDD 91
[2025-11-09T15:16:47.829+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO MapPartitionsRDD: Removing RDD 101 from persistence list
[2025-11-09T15:16:47.829+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BlockManager: Removing RDD 101
[2025-11-09T15:16:47.830+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseHoodieWriteClient: Async cleaner has been spawned. Waiting for it to finish
[2025-11-09T15:16:47.830+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AsyncCleanerService: Waiting for async clean service to finish
[2025-11-09T15:16:47.830+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseHoodieWriteClient: Async cleaner has finished
[2025-11-09T15:16:47.831+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.838+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:47.845+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.845+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.849+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__commit__COMPLETED__20251109151647629]}
[2025-11-09T15:16:47.850+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.857+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:47.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:47.866+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:47.873+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:47.878+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__deltacommit__COMPLETED__20251109151647428]}
[2025-11-09T15:16:47.879+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:47.879+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:47.879+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:47.879+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:47.879+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseHoodieWriteClient: Start to archive synchronously.
[2025-11-09T15:16:47.884+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__commit__COMPLETED__20251109151647629]}
[2025-11-09T15:16:47.885+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.892+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-11-09T15:16:47.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-11-09T15:16:47.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:47.905+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:47.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-11-09T15:16:47.918+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__deltacommit__COMPLETED__20251109151647428]}
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieTimelineArchiver: No Instants to archive
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FileSystemViewManager: Creating remote view for basePath s3a://huditest/silver/table_name=customers. Server=863c17993851:34171, Timeout=300
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=customers
[2025-11-09T15:16:47.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:47.924+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:47.932+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__commit__COMPLETED__20251109151647629]}
[2025-11-09T15:16:47.933+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO RemoteHoodieTableFileSystemView: Sending request : (http://863c17993851:34171/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fhuditest%2Fsilver%2Ftable_name%3Dcustomers&lastinstantts=20251109151634225&timelinehash=843aa3cb22be4d6a211355e7da617aba41507e20dc42a9e00fa715e5a47524b9)
[2025-11-09T15:16:47.941+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__commit__COMPLETED__20251109151647629]}
[2025-11-09T15:16:47.941+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:47.948+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:47.953+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__commit__COMPLETED__20251109151647629]}
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151634225__deltacommit__COMPLETED__20251109151647428]}
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Commit 20251109151634225 successful!
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Compaction Scheduled is Optional.empty
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-11-09T15:16:47.959+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Clustering Scheduled is Optional.empty
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Is Async Compaction Enabled ? false
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO HoodieSparkSqlWriter$: Closing write client
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO BaseHoodieClient: Stopping Timeline service !!
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO EmbeddedTimelineService: Closing Timeline server
[2025-11-09T15:16:47.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TimelineService: Closing Timeline Service
[2025-11-09T15:16:47.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO Javalin: Stopping Javalin ...
[2025-11-09T15:16:47.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO Javalin: Javalin has stopped
[2025-11-09T15:16:47.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO TimelineService: Closed Timeline Service
[2025-11-09T15:16:47.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO EmbeddedTimelineService: Closed Timeline server
[2025-11-09T15:16:47.981+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:47 INFO AsyncCleanerService: Shutting down async clean service...
[2025-11-09T15:16:48.020+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO CodeGenerator: Code generated in 8.65153 ms
[2025-11-09T15:16:48.023+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 12
[2025-11-09T15:16:48.023+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Got map stage job 25 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
[2025-11-09T15:16:48.023+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Final stage: ShuffleMapStage 49 (count at NativeMethodAccessorImpl.java:0)
[2025-11-09T15:16:48.023+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:48.024+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:48.024+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-09T15:16:48.026+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 15.9 KiB, free 432.9 MiB)
[2025-11-09T15:16:48.028+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 432.9 MiB)
[2025-11-09T15:16:48.029+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 863c17993851:44651 (size: 8.4 KiB, free: 434.0 MiB)
[2025-11-09T15:16:48.029+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:48.030+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:48.030+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Adding task set 49.0 with 4 tasks resource profile 0
[2025-11-09T15:16:48.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 174) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 10185 bytes)
[2025-11-09T15:16:48.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 175) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 10066 bytes)
[2025-11-09T15:16:48.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 2.0 in stage 49.0 (TID 176) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 10246 bytes)
[2025-11-09T15:16:48.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 3.0 in stage 49.0 (TID 177) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 10027 bytes)
[2025-11-09T15:16:48.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on 172.19.0.9:35743 (size: 8.4 KiB, free: 434.0 MiB)
[2025-11-09T15:16:48.099+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 174) in 68 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:48.101+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 3.0 in stage 49.0 (TID 177) in 69 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:48.102+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 175) in 71 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:48.109+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 2.0 in stage 49.0 (TID 176) in 78 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:48.110+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2025-11-09T15:16:48.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: ShuffleMapStage 49 (count at NativeMethodAccessorImpl.java:0) finished in 0.086 s
[2025-11-09T15:16:48.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:48.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:48.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: waiting: Set()
[2025-11-09T15:16:48.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:48.122+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
[2025-11-09T15:16:48.123+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Got job 26 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-11-09T15:16:48.124+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Final stage: ResultStage 51 (count at NativeMethodAccessorImpl.java:0)
[2025-11-09T15:16:48.124+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)
[2025-11-09T15:16:48.124+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:48.124+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-11-09T15:16:48.127+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 12.1 KiB, free 432.9 MiB)
[2025-11-09T15:16:48.142+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 432.9 MiB)
[2025-11-09T15:16:48.144+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 863c17993851:44651 (size: 5.8 KiB, free: 434.0 MiB)
[2025-11-09T15:16:48.145+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 863c17993851:44651 in memory (size: 36.0 KiB, free: 434.0 MiB)
[2025-11-09T15:16:48.145+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:48.146+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_32_piece0 on 172.19.0.9:35743 in memory (size: 36.0 KiB, free: 434.0 MiB)
[2025-11-09T15:16:48.146+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:48.146+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
[2025-11-09T15:16:48.148+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 178) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2025-11-09T15:16:48.149+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManager: Removing RDD 91
[2025-11-09T15:16:48.157+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 863c17993851:44651 in memory (size: 162.5 KiB, free: 434.2 MiB)
[2025-11-09T15:16:48.157+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_30_piece0 on 172.19.0.9:35743 in memory (size: 162.5 KiB, free: 434.2 MiB)
[2025-11-09T15:16:48.187+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManager: Removing RDD 101
[2025-11-09T15:16:48.188+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on 172.19.0.9:35743 (size: 5.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:48.193+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 172.19.0.9:56384
[2025-11-09T15:16:48.193+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 863c17993851:44651 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:48.194+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_33_piece0 on 172.19.0.9:35743 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:48.200+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 863c17993851:44651 in memory (size: 123.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.200+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_29_piece0 on 172.19.0.9:35743 in memory (size: 123.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.206+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 863c17993851:44651 in memory (size: 35.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.207+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_31_piece0 on 172.19.0.9:35743 in memory (size: 35.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.208+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 178) in 60 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:48.208+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool
[2025-11-09T15:16:48.208+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: ResultStage 51 (count at NativeMethodAccessorImpl.java:0) finished in 0.083 s
[2025-11-09T15:16:48.209+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:48.209+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
[2025-11-09T15:16:48.209+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Job 26 finished: count at NativeMethodAccessorImpl.java:0, took 0.086654 s
[2025-11-09T15:16:48.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 863c17993851:44651 in memory (size: 8.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Removed broadcast_34_piece0 on 172.19.0.9:35743 in memory (size: 8.4 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.239+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=orders/ as hoodie table s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.432+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.439+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.443+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.443+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.447+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:48.454+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO EmbeddedTimelineService: Starting Timeline service !!
[2025-11-09T15:16:48.454+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO EmbeddedTimelineService: Overriding hostIp to (863c17993851) found in spark-conf. It was null
[2025-11-09T15:16:48.454+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:48.455+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:48.458+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Javalin:
[2025-11-09T15:16:48.458+0000] {spark_submit.py:579} INFO - __                      __ _            __ __
[2025-11-09T15:16:48.458+0000] {spark_submit.py:579} INFO - / /____ _ _   __ ____ _ / /(_)____      / // /
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - __  / // __ `/| | / // __ `// // // __ \    / // /_
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - / /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - \____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - 
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - https://javalin.io/documentation
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - 
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Javalin: Starting Javalin ...
[2025-11-09T15:16:48.459+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1112 days old. Consider checking for a newer version.).
[2025-11-09T15:16:48.460+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 11.0.29+6-post-Debian-1deb11u1
[2025-11-09T15:16:48.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Server: Started @30865ms
[2025-11-09T15:16:48.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Javalin: Listening on http://localhost:39921/
[2025-11-09T15:16:48.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO Javalin: Javalin started in 21ms \o/
[2025-11-09T15:16:48.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TimelineService: Starting Timeline server on port :39921
[2025-11-09T15:16:48.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO EmbeddedTimelineService: Started embedded timeline server at 863c17993851:39921
[2025-11-09T15:16:48.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
[2025-11-09T15:16:48.481+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-11-09T15:16:48.481+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-11-09T15:16:48.492+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.497+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.502+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.506+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:48.506+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO CleanerUtils: Cleaned failed attempts if any
[2025-11-09T15:16:48.506+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.512+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:48.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.526+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.531+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.536+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.541+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:48.541+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:48.541+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BaseHoodieWriteClient: Generate a new instant time: 20251109151648443 action: commit
[2025-11-09T15:16:48.542+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Creating a new instant [==>20251109151648443__commit__REQUESTED]
[2025-11-09T15:16:48.560+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.566+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.571+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.572+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.574+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:48.575+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.579+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:48.585+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:48.587+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
[2025-11-09T15:16:48.591+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:48.592+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=orders//.hoodie/metadata as hoodie table s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:48.828+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:48.834+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:48.842+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:48.842+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:48.854+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-11-09T15:16:48.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Got job 27 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-11-09T15:16:48.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Final stage: ResultStage 52 (collect at HoodieSparkEngineContext.java:116)
[2025-11-09T15:16:48.856+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:48.856+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:48.856+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[120] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-11-09T15:16:48.859+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:48.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.2 MiB)
[2025-11-09T15:16:48.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 863c17993851:44651 (size: 36.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.863+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:48.863+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[120] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:48.864+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
[2025-11-09T15:16:48.865+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 179) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7368 bytes)
[2025-11-09T15:16:48.878+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on 172.19.0.9:35743 (size: 36.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:48.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 179) in 27 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:48.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool
[2025-11-09T15:16:48.894+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: ResultStage 52 (collect at HoodieSparkEngineContext.java:116) finished in 0.037 s
[2025-11-09T15:16:48.894+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:48.894+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
[2025-11-09T15:16:48.895+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Job 27 finished: collect at HoodieSparkEngineContext.java:116, took 0.040010 s
[2025-11-09T15:16:48.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:48.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
[2025-11-09T15:16:48.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
[2025-11-09T15:16:48.903+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
[2025-11-09T15:16:48.904+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Got job 28 (count at HoodieJavaRDD.java:115) with 1 output partitions
[2025-11-09T15:16:48.904+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Final stage: ResultStage 53 (count at HoodieJavaRDD.java:115)
[2025-11-09T15:16:48.904+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:48.904+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:48.904+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting ResultStage 53 (ParallelCollectionRDD[121] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
[2025-11-09T15:16:48.906+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 3.0 KiB, free 434.2 MiB)
[2025-11-09T15:16:48.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.2 MiB)
[2025-11-09T15:16:48.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 863c17993851:44651 (size: 1866.0 B, free: 434.4 MiB)
[2025-11-09T15:16:48.910+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:48.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (ParallelCollectionRDD[121] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:48.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
[2025-11-09T15:16:48.912+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 180) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7405 bytes)
[2025-11-09T15:16:48.923+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on 172.19.0.9:35743 (size: 1866.0 B, free: 434.4 MiB)
[2025-11-09T15:16:48.929+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 180) in 18 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:48.930+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool
[2025-11-09T15:16:48.930+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: ResultStage 53 (count at HoodieJavaRDD.java:115) finished in 0.025 s
[2025-11-09T15:16:48.930+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:48.930+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
[2025-11-09T15:16:48.931+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Job 28 finished: count at HoodieJavaRDD.java:115, took 0.026984 s
[2025-11-09T15:16:48.931+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
[2025-11-09T15:16:48.939+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
[2025-11-09T15:16:48.968+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
[2025-11-09T15:16:48.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Got job 29 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
[2025-11-09T15:16:48.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Final stage: ResultStage 54 (foreach at HoodieSparkEngineContext.java:155)
[2025-11-09T15:16:48.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:48.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:48.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting ResultStage 54 (ParallelCollectionRDD[122] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
[2025-11-09T15:16:48.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 383.5 KiB, free 433.9 MiB)
[2025-11-09T15:16:48.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 127.0 KiB, free 433.7 MiB)
[2025-11-09T15:16:48.983+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 863c17993851:44651 (size: 127.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:48.983+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:48.984+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (ParallelCollectionRDD[122] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:48.984+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
[2025-11-09T15:16:48.986+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 181) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7260 bytes)
[2025-11-09T15:16:48.997+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:48 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on 172.19.0.9:35743 (size: 127.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.038+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 181) in 52 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:49.038+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool
[2025-11-09T15:16:49.039+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: ResultStage 54 (foreach at HoodieSparkEngineContext.java:155) finished in 0.068 s
[2025-11-09T15:16:49.040+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:49.040+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
[2025-11-09T15:16:49.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 29 finished: foreach at HoodieSparkEngineContext.java:155, took 0.072664 s
[2025-11-09T15:16:49.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:49.042+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:49.042+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-11-09T15:16:49.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:49.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:49.043+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-11-09T15:16:49.051+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:49.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:49.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.062+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.077+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:49.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.094+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:49.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:49.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
[2025-11-09T15:16:49.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.105+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.114+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:49.121+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO CleanerUtils: Cleaned failed attempts if any
[2025-11-09T15:16:49.122+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.130+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.139+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.140+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.145+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-11-09T15:16:49.156+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.165+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:49.166+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:49.166+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
[2025-11-09T15:16:49.167+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
[2025-11-09T15:16:49.198+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.209+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.217+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.218+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.223+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20251109151649182]}
[2025-11-09T15:16:49.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.235+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:49.235+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:49.236+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-11-09T15:16:49.236+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-11-09T15:16:49.236+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
[2025-11-09T15:16:49.258+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-11-09T15:16:49.265+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
[2025-11-09T15:16:49.266+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Registering RDD 126 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 13
[2025-11-09T15:16:49.266+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Got job 30 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
[2025-11-09T15:16:49.266+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Final stage: ResultStage 56 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
[2025-11-09T15:16:49.266+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
[2025-11-09T15:16:49.267+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 55)
[2025-11-09T15:16:49.267+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[126] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
[2025-11-09T15:16:49.269+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 8.5 KiB, free 433.7 MiB)
[2025-11-09T15:16:49.281+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.7 MiB)
[2025-11-09T15:16:49.285+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManager: Removing RDD 80
[2025-11-09T15:16:49.286+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 863c17993851:44651 (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.287+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:49.291+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[126] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:49.292+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
[2025-11-09T15:16:49.294+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 182) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7503 bytes)
[2025-11-09T15:16:49.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 863c17993851:44651 in memory (size: 5.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_35_piece0 on 172.19.0.9:35743 in memory (size: 5.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.305+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 863c17993851:44651 in memory (size: 127.0 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.306+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_38_piece0 on 172.19.0.9:35743 in memory (size: 127.0 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on 172.19.0.9:35743 (size: 4.7 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.331+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManager: Removing RDD 44
[2025-11-09T15:16:49.338+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 863c17993851:44651 in memory (size: 1866.0 B, free: 434.4 MiB)
[2025-11-09T15:16:49.339+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_37_piece0 on 172.19.0.9:35743 in memory (size: 1866.0 B, free: 434.4 MiB)
[2025-11-09T15:16:49.342+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManager: Removing RDD 62
[2025-11-09T15:16:49.344+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 182) in 50 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:49.344+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool
[2025-11-09T15:16:49.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: ShuffleMapStage 55 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.077 s
[2025-11-09T15:16:49.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:49.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:49.345+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: waiting: Set(ResultStage 56)
[2025-11-09T15:16:49.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:49.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[129] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
[2025-11-09T15:16:49.346+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManager: Removing RDD 70
[2025-11-09T15:16:49.348+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 7.3 KiB, free 434.2 MiB)
[2025-11-09T15:16:49.352+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 863c17993851:44651 in memory (size: 36.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.352+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Removed broadcast_36_piece0 on 172.19.0.9:35743 in memory (size: 36.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.353+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 434.4 MiB)
[2025-11-09T15:16:49.354+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 863c17993851:44651 (size: 3.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.355+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:49.355+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[129] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:49.356+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
[2025-11-09T15:16:49.357+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 183) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:49.370+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on 172.19.0.9:35743 (size: 3.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:49.374+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 172.19.0.9:56384
[2025-11-09T15:16:49.384+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 183) in 26 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:49.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool
[2025-11-09T15:16:49.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: ResultStage 56 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.038 s
[2025-11-09T15:16:49.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:49.385+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
[2025-11-09T15:16:49.386+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 30 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.120111 s
[2025-11-09T15:16:49.397+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-11-09T15:16:49.398+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
[2025-11-09T15:16:49.419+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-11-09T15:16:49.420+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Got job 31 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-11-09T15:16:49.420+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Final stage: ResultStage 58 (collect at HoodieJavaRDD.java:177)
[2025-11-09T15:16:49.420+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)
[2025-11-09T15:16:49.421+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:49.421+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[132] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-11-09T15:16:49.430+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 357.4 KiB, free 434.0 MiB)
[2025-11-09T15:16:49.432+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 123.7 KiB, free 433.9 MiB)
[2025-11-09T15:16:49.433+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 863c17993851:44651 (size: 123.7 KiB, free: 434.3 MiB)
[2025-11-09T15:16:49.434+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:49.434+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[132] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:49.435+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0
[2025-11-09T15:16:49.436+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 184) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:49.448+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on 172.19.0.9:35743 (size: 123.7 KiB, free: 434.3 MiB)
[2025-11-09T15:16:49.614+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added rdd_131_0 in memory on 172.19.0.9:35743 (size: 290.0 B, free: 434.3 MiB)
[2025-11-09T15:16:49.621+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 184) in 184 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:49.621+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool
[2025-11-09T15:16:49.622+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: ResultStage 58 (collect at HoodieJavaRDD.java:177) finished in 0.200 s
[2025-11-09T15:16:49.622+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:49.622+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
[2025-11-09T15:16:49.622+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 31 finished: collect at HoodieJavaRDD.java:177, took 0.203061 s
[2025-11-09T15:16:49.623+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
[2025-11-09T15:16:49.623+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
[2025-11-09T15:16:49.648+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-11-09T15:16:49.649+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Got job 32 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-11-09T15:16:49.649+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Final stage: ResultStage 59 (collect at HoodieSparkEngineContext.java:150)
[2025-11-09T15:16:49.649+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:49.649+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:49.649+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[134] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-11-09T15:16:49.653+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 100.7 KiB, free 433.8 MiB)
[2025-11-09T15:16:49.656+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.8 MiB)
[2025-11-09T15:16:49.656+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 863c17993851:44651 (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.656+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:49.657+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[134] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:49.657+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0
[2025-11-09T15:16:49.659+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 185) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-11-09T15:16:49.670+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on 172.19.0.9:35743 (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.684+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 185) in 26 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:49.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool
[2025-11-09T15:16:49.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: ResultStage 59 (collect at HoodieSparkEngineContext.java:150) finished in 0.035 s
[2025-11-09T15:16:49.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:49.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
[2025-11-09T15:16:49.685+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 32 finished: collect at HoodieSparkEngineContext.java:150, took 0.036989 s
[2025-11-09T15:16:49.686+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
[2025-11-09T15:16:49.686+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-11-09T15:16:49.709+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
[2025-11-09T15:16:49.709+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
[2025-11-09T15:16:49.709+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
[2025-11-09T15:16:49.730+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-11-09T15:16:49.731+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Got job 33 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-11-09T15:16:49.731+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Final stage: ResultStage 60 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-11-09T15:16:49.731+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:49.732+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:49.732+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[136] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-11-09T15:16:49.736+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 100.9 KiB, free 433.7 MiB)
[2025-11-09T15:16:49.738+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.6 MiB)
[2025-11-09T15:16:49.739+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 863c17993851:44651 (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.740+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:49.740+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[136] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:49.740+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
[2025-11-09T15:16:49.741+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 186) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-11-09T15:16:49.755+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on 172.19.0.9:35743 (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:49.796+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 186) in 55 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:49.797+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool
[2025-11-09T15:16:49.797+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: ResultStage 60 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.065 s
[2025-11-09T15:16:49.797+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:49.797+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
[2025-11-09T15:16:49.798+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO DAGScheduler: Job 33 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.067438 s
[2025-11-09T15:16:49.824+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/.temp/00000000000000010
[2025-11-09T15:16:49.825+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.830+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.836+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.836+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.842+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:49.849+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.856+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:49.856+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:49.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:49.868+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:49.945+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: MDT s3a://huditest/silver/table_name=orders partition FILES has been enabled
[2025-11-09T15:16:49.946+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:49.953+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:49.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:49.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:49.966+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.973+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:49.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:49.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:49.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:49.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 1049 in ms
[2025-11-09T15:16:49.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.986+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:49.994+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.994+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:49.999+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:49 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.007+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.015+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.015+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.015+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.024+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.037+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.045+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.058+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.058+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
[2025-11-09T15:16:50.063+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.069+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.075+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.076+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.081+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.088+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.094+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BaseHoodieWriteClient: Cleaner started
[2025-11-09T15:16:50.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.100+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.107+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.107+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.111+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.119+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
[2025-11-09T15:16:50.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
[2025-11-09T15:16:50.130+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.131+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.139+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.144+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.145+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.149+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.156+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.161+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.166+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:50.166+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
[2025-11-09T15:16:50.167+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.174+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.180+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.180+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.184+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.191+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.196+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.196+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.197+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
[2025-11-09T15:16:50.197+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.197+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.203+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.209+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.209+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:50.213+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.219+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.225+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:50.225+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:50.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.231+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.237+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:50.246+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.251+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:50.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTimelineArchiver: No Instants to archive
[2025-11-09T15:16:50.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
[2025-11-09T15:16:50.252+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.258+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:50.265+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.265+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.271+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.278+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.283+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.283+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:50.283+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:50.283+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:50.284+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:50.284+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AsyncCleanerService: Starting async clean service with instant time 20251109151650283...
[2025-11-09T15:16:50.284+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-11-09T15:16:50.284+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.292+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:50.296+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-11-09T15:16:50.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 137 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 15
[2025-11-09T15:16:50.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 141 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 14
[2025-11-09T15:16:50.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Got job 34 (countByKey at HoodieJavaPairRDD.java:105) with 4 output partitions
[2025-11-09T15:16:50.298+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Final stage: ResultStage 63 (countByKey at HoodieJavaPairRDD.java:105)
[2025-11-09T15:16:50.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)
[2025-11-09T15:16:50.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 62)
[2025-11-09T15:16:50.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.299+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ShuffleMapStage 61 (MapPartitionsRDD[137] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:50.303+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 76.2 KiB, free 433.6 MiB)
[2025-11-09T15:16:50.304+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:50.304+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.319+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 433.5 MiB)
[2025-11-09T15:16:50.319+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 863c17993851:44651 (size: 27.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.321+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.322+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 863c17993851:44651 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.322+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 61 (MapPartitionsRDD[137] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.322+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 61.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.322+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_39_piece0 on 172.19.0.9:35743 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.323+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 187) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 10185 bytes)
[2025-11-09T15:16:50.324+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 61.0 (TID 188) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 10066 bytes)
[2025-11-09T15:16:50.324+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 61.0 (TID 189) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 10246 bytes)
[2025-11-09T15:16:50.324+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 61.0 (TID 190) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 10027 bytes)
[2025-11-09T15:16:50.325+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:50.327+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 863c17993851:44651 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_42_piece0 on 172.19.0.9:35743 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.334+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 863c17993851:44651 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.335+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_43_piece0 on 172.19.0.9:35743 in memory (size: 36.0 KiB, free: 434.3 MiB)
[2025-11-09T15:16:50.336+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.336+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on 172.19.0.9:35743 (size: 27.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.337+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.353+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 863c17993851:44651 in memory (size: 123.7 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.354+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_41_piece0 on 172.19.0.9:35743 in memory (size: 123.7 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.358+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManager: Removing RDD 131
[2025-11-09T15:16:50.359+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.363+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 863c17993851:44651 in memory (size: 3.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.364+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_40_piece0 on 172.19.0.9:35743 in memory (size: 3.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.370+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.375+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:50.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:50.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:50.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:50.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BaseHoodieWriteClient: Cleaner started
[2025-11-09T15:16:50.376+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.383+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:50.391+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.392+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.397+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:50.397+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.406+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:50.408+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 3.0 in stage 61.0 (TID 190) in 84 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:50.411+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 1.0 in stage 61.0 (TID 188) in 88 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:50.412+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 187) in 89 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:50.413+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 2.0 in stage 61.0 (TID 189) in 89 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:50.413+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.414+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ShuffleMapStage 61 (mapToPair at HoodieJavaRDD.java:149) finished in 0.114 s
[2025-11-09T15:16:50.415+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:50.415+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:50.415+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: waiting: Set(ResultStage 63, ShuffleMapStage 62)
[2025-11-09T15:16:50.415+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:50.415+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:50.415+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.416+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[141] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:50.419+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 26.9 KiB, free 434.3 MiB)
[2025-11-09T15:16:50.422+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:50.423+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 434.3 MiB)
[2025-11-09T15:16:50.424+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 863c17993851:44651 (size: 12.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.425+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.426+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[141] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.426+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.427+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 191) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.428+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 192) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.428+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 193) (172.19.0.9, executor 0, partition 2, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.429+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 194) (172.19.0.9, executor 0, partition 3, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.430+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:50.435+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:50.436+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:50.436+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:50.436+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:50.436+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:50.436+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :20251109151650283
[2025-11-09T15:16:50.441+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on 172.19.0.9:35743 (size: 12.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.442+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__REQUESTED__20251109151648550]}
[2025-11-09T15:16:50.448+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 172.19.0.9:56384
[2025-11-09T15:16:50.465+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added rdd_139_3 in memory on 172.19.0.9:35743 (size: 2.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.466+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added rdd_139_2 in memory on 172.19.0.9:35743 (size: 2.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.467+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added rdd_139_0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.467+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added rdd_139_1 in memory on 172.19.0.9:35743 (size: 3.2 KiB, free: 434.3 MiB)
[2025-11-09T15:16:50.475+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 194) in 47 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:50.478+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 192) in 50 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:50.478+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 191) in 50 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:50.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 193) in 50 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:50.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ShuffleMapStage 62 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.063 s
[2025-11-09T15:16:50.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:50.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:50.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: waiting: Set(ResultStage 63)
[2025-11-09T15:16:50.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:50.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ResultStage 63 (ShuffledRDD[142] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:50.481+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 5.5 KiB, free 434.3 MiB)
[2025-11-09T15:16:50.483+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:50.484+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.484+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.485+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 63 (ShuffledRDD[142] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.486+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 63.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.487+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 195) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.488+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 63.0 (TID 196) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.489+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 63.0 (TID 197) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.489+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 63.0 (TID 198) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.502+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.3 MiB)
[2025-11-09T15:16:50.507+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 172.19.0.9:56384
[2025-11-09T15:16:50.516+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 1.0 in stage 63.0 (TID 196) in 27 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:50.518+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 3.0 in stage 63.0 (TID 198) in 29 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:50.519+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 2.0 in stage 63.0 (TID 197) in 29 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:50.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 195) in 32 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:50.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.520+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ResultStage 63 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.040 s
[2025-11-09T15:16:50.521+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:50.521+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
[2025-11-09T15:16:50.521+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 34 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.224967 s
[2025-11-09T15:16:50.566+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-11-09T15:16:50.568+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Got job 35 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-11-09T15:16:50.568+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Final stage: ResultStage 64 (collect at HoodieSparkEngineContext.java:150)
[2025-11-09T15:16:50.568+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:50.568+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:50.569+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[144] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-11-09T15:16:50.585+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 453.4 KiB, free 433.8 MiB)
[2025-11-09T15:16:50.589+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 158.2 KiB, free 433.7 MiB)
[2025-11-09T15:16:50.590+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 863c17993851:44651 (size: 158.2 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.590+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.591+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[144] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:50.591+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0
[2025-11-09T15:16:50.593+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 199) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7249 bytes)
[2025-11-09T15:16:50.605+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on 172.19.0.9:35743 (size: 158.2 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.623+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 199) in 31 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:50.623+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.624+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ResultStage 64 (collect at HoodieSparkEngineContext.java:150) finished in 0.053 s
[2025-11-09T15:16:50.625+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:50.625+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
[2025-11-09T15:16:50.625+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 35 finished: collect at HoodieSparkEngineContext.java:150, took 0.057916 s
[2025-11-09T15:16:50.655+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 863c17993851:44651 in memory (size: 27.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.656+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_44_piece0 on 172.19.0.9:35743 in memory (size: 27.7 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.661+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 863c17993851:44651 in memory (size: 158.2 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.663+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_47_piece0 on 172.19.0.9:35743 in memory (size: 158.2 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.668+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 863c17993851:44651 in memory (size: 12.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.681+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_45_piece0 on 172.19.0.9:35743 in memory (size: 12.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.686+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.687+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_46_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.705+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-11-09T15:16:50.705+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Got job 36 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-11-09T15:16:50.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Final stage: ResultStage 65 (collect at HoodieSparkEngineContext.java:116)
[2025-11-09T15:16:50.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:50.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:50.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[146] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-11-09T15:16:50.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 453.2 KiB, free 434.0 MiB)
[2025-11-09T15:16:50.725+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 157.9 KiB, free 433.8 MiB)
[2025-11-09T15:16:50.726+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 863c17993851:44651 (size: 157.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.726+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.726+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[146] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:50.727+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
[2025-11-09T15:16:50.728+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 200) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7246 bytes)
[2025-11-09T15:16:50.742+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on 172.19.0.9:35743 (size: 157.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.759+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 200) in 31 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:50.760+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.760+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ResultStage 65 (collect at HoodieSparkEngineContext.java:116) finished in 0.054 s
[2025-11-09T15:16:50.760+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:50.760+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
[2025-11-09T15:16:50.761+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 36 finished: collect at HoodieSparkEngineContext.java:116, took 0.055956 s
[2025-11-09T15:16:50.763+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkHoodieBloomIndexHelper: Input parallelism: 4, Index parallelism: 4
[2025-11-09T15:16:50.771+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Starting job: countByKey at SparkHoodieBloomIndexHelper.java:197
[2025-11-09T15:16:50.772+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 149 (countByKey at SparkHoodieBloomIndexHelper.java:197) as input to shuffle 16
[2025-11-09T15:16:50.773+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Got job 37 (countByKey at SparkHoodieBloomIndexHelper.java:197) with 4 output partitions
[2025-11-09T15:16:50.773+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Final stage: ResultStage 68 (countByKey at SparkHoodieBloomIndexHelper.java:197)
[2025-11-09T15:16:50.773+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)
[2025-11-09T15:16:50.773+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 67)
[2025-11-09T15:16:50.774+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ShuffleMapStage 67 (MapPartitionsRDD[149] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-11-09T15:16:50.777+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.6 KiB, free 433.8 MiB)
[2025-11-09T15:16:50.781+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 433.8 MiB)
[2025-11-09T15:16:50.781+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 863c17993851:44651 (size: 13.5 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.782+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.782+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 67 (MapPartitionsRDD[149] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.783+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 67.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.785+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 201) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.786+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 67.0 (TID 202) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.786+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 67.0 (TID 203) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.786+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 67.0 (TID 204) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.797+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on 172.19.0.9:35743 (size: 13.5 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.809+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 2.0 in stage 67.0 (TID 203) in 24 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:50.812+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 201) in 27 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:50.812+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 3.0 in stage 67.0 (TID 204) in 27 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:50.812+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 1.0 in stage 67.0 (TID 202) in 27 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:50.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ShuffleMapStage 67 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.037 s
[2025-11-09T15:16:50.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:50.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:50.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: waiting: Set(ResultStage 68)
[2025-11-09T15:16:50.813+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:50.814+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ResultStage 68 (ShuffledRDD[150] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-11-09T15:16:50.815+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 5.5 KiB, free 433.8 MiB)
[2025-11-09T15:16:50.818+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.8 MiB)
[2025-11-09T15:16:50.819+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.820+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.821+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 68 (ShuffledRDD[150] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.821+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 68.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.822+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 205) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.823+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 206) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.823+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 207) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.823+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 68.0 (TID 208) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:50.835+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.840+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 172.19.0.9:56384
[2025-11-09T15:16:50.849+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 207) in 25 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:50.851+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 3.0 in stage 68.0 (TID 208) in 28 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:50.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 205) in 29 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:50.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 206) in 29 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:50.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.852+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ResultStage 68 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.038 s
[2025-11-09T15:16:50.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:50.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
[2025-11-09T15:16:50.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Job 37 finished: countByKey at SparkHoodieBloomIndexHelper.java:197, took 0.081944 s
[2025-11-09T15:16:50.854+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BucketizedBloomCheckPartitioner: TotalBuckets 0, min_buckets/partition 1
[2025-11-09T15:16:50.880+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapPartitionsRDD: Removing RDD 139 from persistence list
[2025-11-09T15:16:50.881+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManager: Removing RDD 139
[2025-11-09T15:16:50.882+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapPartitionsRDD: Removing RDD 157 from persistence list
[2025-11-09T15:16:50.882+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManager: Removing RDD 157
[2025-11-09T15:16:50.883+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:50.889+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:50.897+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-11-09T15:16:50.900+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 151 (mapToPair at SparkHoodieBloomIndexHelper.java:166) as input to shuffle 20
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 157 (flatMapToPair at SparkHoodieBloomIndexHelper.java:177) as input to shuffle 18
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 158 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 17
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Registering RDD 167 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 19
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Got job 38 (countByKey at HoodieJavaPairRDD.java:105) with 4 output partitions
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Final stage: ResultStage 74 (countByKey at HoodieJavaPairRDD.java:105)
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
[2025-11-09T15:16:50.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 73)
[2025-11-09T15:16:50.902+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ShuffleMapStage 72 (MapPartitionsRDD[158] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:50.905+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 26.7 KiB, free 433.7 MiB)
[2025-11-09T15:16:50.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.7 MiB)
[2025-11-09T15:16:50.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 863c17993851:44651 (size: 12.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 72 (MapPartitionsRDD[158] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 72.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 72.0 (TID 209) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 72.0 (TID 210) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 72.0 (TID 211) (172.19.0.9, executor 0, partition 2, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.909+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 72.0 (TID 212) (172.19.0.9, executor 0, partition 3, NODE_LOCAL, 7174 bytes)
[2025-11-09T15:16:50.920+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on 172.19.0.9:35743 (size: 12.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.924+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 172.19.0.9:56384
[2025-11-09T15:16:50.938+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 1.0 in stage 72.0 (TID 210) in 29 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:50.941+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 0.0 in stage 72.0 (TID 209) in 31 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:50.946+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 2.0 in stage 72.0 (TID 211) in 36 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:50.946+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Finished task 3.0 in stage 72.0 (TID 212) in 37 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:50.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Removed TaskSet 72.0, whose tasks have all completed, from pool
[2025-11-09T15:16:50.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: ShuffleMapStage 72 (mapToPair at HoodieJavaRDD.java:149) finished in 0.043 s
[2025-11-09T15:16:50.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:50.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:50.948+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: waiting: Set(ResultStage 74, ShuffleMapStage 73)
[2025-11-09T15:16:50.948+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:50.948+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting ShuffleMapStage 73 (MapPartitionsRDD[167] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:50.949+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 10.0 KiB, free 433.7 MiB)
[2025-11-09T15:16:50.961+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 433.7 MiB)
[2025-11-09T15:16:50.962+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 863c17993851:44651 (size: 5.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:50.962+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:50.963+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 863c17993851:44651 in memory (size: 157.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.963+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 73 (MapPartitionsRDD[167] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:50.963+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSchedulerImpl: Adding task set 73.0 with 4 tasks resource profile 0
[2025-11-09T15:16:50.964+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_48_piece0 on 172.19.0.9:35743 in memory (size: 157.9 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.965+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 213) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:50.965+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 1.0 in stage 73.0 (TID 214) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:50.966+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 2.0 in stage 73.0 (TID 215) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:50.966+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO TaskSetManager: Starting task 3.0 in stage 73.0 (TID 216) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:50.970+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.972+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_50_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.978+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 863c17993851:44651 in memory (size: 13.5 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.979+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on 172.19.0.9:35743 (size: 5.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.980+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO BlockManagerInfo: Removed broadcast_49_piece0 on 172.19.0.9:35743 in memory (size: 13.5 KiB, free: 434.4 MiB)
[2025-11-09T15:16:50.997+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:50 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 172.19.0.9:56384
[2025-11-09T15:16:51.003+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 172.19.0.9:56384
[2025-11-09T15:16:51.016+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added rdd_165_1 in memory on 172.19.0.9:35743 (size: 3.2 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.016+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added rdd_165_3 in memory on 172.19.0.9:35743 (size: 2.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.017+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added rdd_165_2 in memory on 172.19.0.9:35743 (size: 2.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.017+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added rdd_165_0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.024+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 1.0 in stage 73.0 (TID 214) in 59 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:51.030+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 2.0 in stage 73.0 (TID 215) in 65 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:51.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 213) in 66 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:51.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 3.0 in stage 73.0 (TID 216) in 65 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:51.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool
[2025-11-09T15:16:51.031+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: ShuffleMapStage 73 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.083 s
[2025-11-09T15:16:51.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:51.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:51.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: waiting: Set(ResultStage 74)
[2025-11-09T15:16:51.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:51.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting ResultStage 74 (ShuffledRDD[168] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:51.034+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 5.5 KiB, free 434.3 MiB)
[2025-11-09T15:16:51.036+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.3 MiB)
[2025-11-09T15:16:51.037+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.038+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:51.039+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 74 (ShuffledRDD[168] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:51.039+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Adding task set 74.0 with 4 tasks resource profile 0
[2025-11-09T15:16:51.040+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 217) (172.19.0.9, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:51.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 218) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:51.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 2.0 in stage 74.0 (TID 219) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:51.041+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 3.0 in stage 74.0 (TID 220) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:51.052+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.057+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 172.19.0.9:56384
[2025-11-09T15:16:51.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 3.0 in stage 74.0 (TID 220) in 23 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:51.067+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 218) in 26 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:51.067+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 2.0 in stage 74.0 (TID 219) in 27 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:51.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 217) in 30 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:51.070+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool
[2025-11-09T15:16:51.071+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: ResultStage 74 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.038 s
[2025-11-09T15:16:51.071+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:51.071+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
[2025-11-09T15:16:51.072+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 38 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.173912 s
[2025-11-09T15:16:51.072+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-11-09T15:16:51.112+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-11-09T15:16:51.112+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Got job 39 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
[2025-11-09T15:16:51.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Final stage: ResultStage 75 (collectAsMap at UpsertPartitioner.java:282)
[2025-11-09T15:16:51.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:51.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:51.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[170] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-11-09T15:16:51.125+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 454.2 KiB, free 433.9 MiB)
[2025-11-09T15:16:51.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 159.0 KiB, free 433.7 MiB)
[2025-11-09T15:16:51.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 863c17993851:44651 (size: 159.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:51.129+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[170] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:51.129+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0
[2025-11-09T15:16:51.130+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 221) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7249 bytes)
[2025-11-09T15:16:51.140+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on 172.19.0.9:35743 (size: 159.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.153+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 221) in 23 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:51.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool
[2025-11-09T15:16:51.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: ResultStage 75 (collectAsMap at UpsertPartitioner.java:282) finished in 0.041 s
[2025-11-09T15:16:51.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:51.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
[2025-11-09T15:16:51.154+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 39 finished: collectAsMap at UpsertPartitioner.java:282, took 0.042730 s
[2025-11-09T15:16:51.155+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:51.160+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:51.160+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO UpsertPartitioner: Total Buckets: 1
[2025-11-09T15:16:51.160+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/20251109151648443.commit.requested
[2025-11-09T15:16:51.180+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/20251109151648443.inflight
[2025-11-09T15:16:51.194+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-11-09T15:16:51.195+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseCommitActionExecutor: Auto commit disabled for 20251109151648443
[2025-11-09T15:16:51.198+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1050
[2025-11-09T15:16:51.200+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Registering RDD 171 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 21
[2025-11-09T15:16:51.200+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Got job 40 (count at HoodieSparkSqlWriter.scala:1050) with 1 output partitions
[2025-11-09T15:16:51.201+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Final stage: ResultStage 81 (count at HoodieSparkSqlWriter.scala:1050)
[2025-11-09T15:16:51.201+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 80)
[2025-11-09T15:16:51.201+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 80)
[2025-11-09T15:16:51.203+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting ShuffleMapStage 80 (MapPartitionsRDD[171] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:51.215+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 459.4 KiB, free 433.3 MiB)
[2025-11-09T15:16:51.227+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 160.8 KiB, free 433.1 MiB)
[2025-11-09T15:16:51.227+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 863c17993851:44651 in memory (size: 5.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.227+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 863c17993851:44651 (size: 160.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:51.227+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:51.228+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[171] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-11-09T15:16:51.228+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Adding task set 80.0 with 4 tasks resource profile 0
[2025-11-09T15:16:51.228+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_52_piece0 on 172.19.0.9:35743 in memory (size: 5.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 222) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:51.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 223) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:51.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 2.0 in stage 80.0 (TID 224) (172.19.0.9, executor 0, partition 2, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:51.230+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 3.0 in stage 80.0 (TID 225) (172.19.0.9, executor 0, partition 3, PROCESS_LOCAL, 7237 bytes)
[2025-11-09T15:16:51.233+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 863c17993851:44651 in memory (size: 12.8 KiB, free: 434.1 MiB)
[2025-11-09T15:16:51.234+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_51_piece0 on 172.19.0.9:35743 in memory (size: 12.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.238+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 863c17993851:44651 in memory (size: 159.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.239+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_54_piece0 on 172.19.0.9:35743 in memory (size: 159.0 KiB, free: 434.4 MiB)
[2025-11-09T15:16:51.254+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on 172.19.0.9:35743 (size: 160.8 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.254+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.256+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Removed broadcast_53_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.2 MiB)
[2025-11-09T15:16:51.278+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 222) in 49 ms on 172.19.0.9 (executor 0) (1/4)
[2025-11-09T15:16:51.281+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 223) in 51 ms on 172.19.0.9 (executor 0) (2/4)
[2025-11-09T15:16:51.281+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 3.0 in stage 80.0 (TID 225) in 51 ms on 172.19.0.9 (executor 0) (3/4)
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 2.0 in stage 80.0 (TID 224) in 51 ms on 172.19.0.9 (executor 0) (4/4)
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: ShuffleMapStage 80 (mapToPair at HoodieJavaRDD.java:149) finished in 0.078 s
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: waiting: Set(ResultStage 81)
[2025-11-09T15:16:51.282+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:51.283+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[176] at filter at HoodieSparkSqlWriter.scala:1050), which has no missing parents
[2025-11-09T15:16:51.296+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 476.7 KiB, free 433.3 MiB)
[2025-11-09T15:16:51.300+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 168.3 KiB, free 433.2 MiB)
[2025-11-09T15:16:51.301+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 863c17993851:44651 (size: 168.3 KiB, free: 434.1 MiB)
[2025-11-09T15:16:51.302+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:51.302+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[176] at filter at HoodieSparkSqlWriter.scala:1050) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:51.302+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0
[2025-11-09T15:16:51.304+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 226) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:51.312+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on 172.19.0.9:35743 (size: 168.3 KiB, free: 434.1 MiB)
[2025-11-09T15:16:51.324+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 172.19.0.9:56384
[2025-11-09T15:16:51.395+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MarkerHandler: Request: create marker: f989a101-9471-4860-80bb-33a3b51f6a64-0_0-81-226_20251109151648443.parquet.marker.CREATE
[2025-11-09T15:16:51.576+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added rdd_175_0 in memory on 172.19.0.9:35743 (size: 311.0 B, free: 434.1 MiB)
[2025-11-09T15:16:51.580+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 226) in 277 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:51.580+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool
[2025-11-09T15:16:51.581+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: ResultStage 81 (count at HoodieSparkSqlWriter.scala:1050) finished in 0.297 s
[2025-11-09T15:16:51.581+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:51.581+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
[2025-11-09T15:16:51.581+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 40 finished: count at HoodieSparkSqlWriter.scala:1050, took 0.383039 s
[2025-11-09T15:16:51.582+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieSparkSqlWriter$: Proceeding to commit the write.
[2025-11-09T15:16:51.609+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
[2025-11-09T15:16:51.610+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Got job 41 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
[2025-11-09T15:16:51.610+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Final stage: ResultStage 87 (collect at SparkRDDWriteClient.java:103)
[2025-11-09T15:16:51.610+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 86)
[2025-11-09T15:16:51.610+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:51.611+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[177] at map at SparkRDDWriteClient.java:103), which has no missing parents
[2025-11-09T15:16:51.623+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 477.3 KiB, free 432.7 MiB)
[2025-11-09T15:16:51.626+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 168.6 KiB, free 432.5 MiB)
[2025-11-09T15:16:51.626+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 863c17993851:44651 (size: 168.6 KiB, free: 433.9 MiB)
[2025-11-09T15:16:51.627+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:51.627+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[177] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:51.628+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks resource profile 0
[2025-11-09T15:16:51.629+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 227) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-11-09T15:16:51.638+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on 172.19.0.9:35743 (size: 168.6 KiB, free: 433.9 MiB)
[2025-11-09T15:16:51.652+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 227) in 24 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:51.653+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool
[2025-11-09T15:16:51.653+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: ResultStage 87 (collect at SparkRDDWriteClient.java:103) finished in 0.042 s
[2025-11-09T15:16:51.654+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:51.654+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
[2025-11-09T15:16:51.654+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO DAGScheduler: Job 41 finished: collect at SparkRDDWriteClient.java:103, took 0.044960 s
[2025-11-09T15:16:51.654+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseHoodieWriteClient: Committing 20251109151648443 action commit
[2025-11-09T15:16:51.654+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.661+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:51.668+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.669+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.673+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__INFLIGHT__20251109151651170]}
[2025-11-09T15:16:51.673+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.680+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:51.686+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.687+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:51.692+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.699+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:51.705+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.705+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:51.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:51.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:51.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:51.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
[2025-11-09T15:16:51.706+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.710+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:51.715+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.715+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__commit__INFLIGHT__20251109151651170]}
[2025-11-09T15:16:51.721+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.728+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:51.733+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.733+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:51.740+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.745+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:51.750+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.750+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:51.751+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:51.751+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:51.751+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:51.751+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseHoodieWriteClient: Committing 20251109151648443 action commit
[2025-11-09T15:16:51.770+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.777+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:51.781+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.783+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.789+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.795+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.799+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.800+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
[2025-11-09T15:16:51.800+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.807+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:51.814+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:51.814+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:51.819+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.825+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:51.831+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.831+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:51.832+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:51.838+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetadataUtil: Updating at 20251109151648443 from Commit/UPSERT. #partitions_updated=2, #files_added=1
[2025-11-09T15:16:51.854+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:51.854+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:51.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-11-09T15:16:51.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:51.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:51.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-11-09T15:16:51.861+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:51.861+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-11-09T15:16:51.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.868+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.875+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.875+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.880+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.887+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.892+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:51.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:51.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieBackedTableMetadataWriter: New commit at 20251109151648443 being applied to MDT.
[2025-11-09T15:16:51.893+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.899+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.907+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.908+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.912+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.912+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO CleanerUtils: Cleaned failed attempts if any
[2025-11-09T15:16:51.913+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.919+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.926+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.927+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.931+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251109151649698]}
[2025-11-09T15:16:51.938+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.944+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:51.945+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:51.945+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO BaseHoodieWriteClient: Generate a new instant time: 20251109151648443 action: deltacommit
[2025-11-09T15:16:51.945+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Creating a new instant [==>20251109151648443__deltacommit__REQUESTED]
[2025-11-09T15:16:51.970+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.978+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:51.985+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.985+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:51.991+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251109151648443__deltacommit__REQUESTED__20251109151651956]}
[2025-11-09T15:16:51.998+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:51 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:52.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:52.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:52.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-11-09T15:16:52.004+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-11-09T15:16:52.005+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.005+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.019+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-11-09T15:16:52.021+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Registering RDD 188 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 22
[2025-11-09T15:16:52.021+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Got job 42 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
[2025-11-09T15:16:52.022+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Final stage: ResultStage 89 (countByKey at HoodieJavaPairRDD.java:105)
[2025-11-09T15:16:52.022+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)
[2025-11-09T15:16:52.022+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 88)
[2025-11-09T15:16:52.022+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ShuffleMapStage 88 (MapPartitionsRDD[188] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:52.024+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 10.1 KiB, free 432.5 MiB)
[2025-11-09T15:16:52.027+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 432.5 MiB)
[2025-11-09T15:16:52.027+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 863c17993851:44651 (size: 5.5 KiB, free: 433.9 MiB)
[2025-11-09T15:16:52.028+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.028+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 88 (MapPartitionsRDD[188] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.029+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.032+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 228) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7688 bytes)
[2025-11-09T15:16:52.047+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on 172.19.0.9:35743 (size: 5.5 KiB, free: 433.9 MiB)
[2025-11-09T15:16:52.055+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added rdd_186_0 in memory on 172.19.0.9:35743 (size: 354.0 B, free: 433.9 MiB)
[2025-11-09T15:16:52.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 228) in 32 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ShuffleMapStage 88 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.042 s
[2025-11-09T15:16:52.064+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:52.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:52.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: waiting: Set(ResultStage 89)
[2025-11-09T15:16:52.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:52.065+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ResultStage 89 (ShuffledRDD[189] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-11-09T15:16:52.067+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 5.5 KiB, free 432.5 MiB)
[2025-11-09T15:16:52.082+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 432.5 MiB)
[2025-11-09T15:16:52.083+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 863c17993851:44651 (size: 3.1 KiB, free: 433.9 MiB)
[2025-11-09T15:16:52.083+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 863c17993851:44651 in memory (size: 168.3 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.084+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.084+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (ShuffledRDD[189] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.085+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.086+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_56_piece0 on 172.19.0.9:35743 in memory (size: 168.3 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.087+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 229) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:52.089+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 863c17993851:44651 in memory (size: 168.6 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.091+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_57_piece0 on 172.19.0.9:35743 in memory (size: 168.6 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.095+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 863c17993851:44651 in memory (size: 160.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:52.097+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_55_piece0 on 172.19.0.9:35743 in memory (size: 160.8 KiB, free: 434.4 MiB)
[2025-11-09T15:16:52.099+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on 172.19.0.9:35743 (size: 3.1 KiB, free: 434.4 MiB)
[2025-11-09T15:16:52.113+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 172.19.0.9:56384
[2025-11-09T15:16:52.127+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 229) in 40 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ResultStage 89 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.063 s
[2025-11-09T15:16:52.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:52.128+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
[2025-11-09T15:16:52.129+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 42 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.108994 s
[2025-11-09T15:16:52.129+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-11-09T15:16:52.162+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-11-09T15:16:52.163+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Got job 43 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
[2025-11-09T15:16:52.163+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Final stage: ResultStage 90 (collectAsMap at UpsertPartitioner.java:282)
[2025-11-09T15:16:52.163+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:52.163+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:52.164+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[191] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-11-09T15:16:52.173+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 352.5 KiB, free 434.0 MiB)
[2025-11-09T15:16:52.176+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 123.7 KiB, free 433.9 MiB)
[2025-11-09T15:16:52.177+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 863c17993851:44651 (size: 123.7 KiB, free: 434.3 MiB)
[2025-11-09T15:16:52.178+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.178+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[191] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.178+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.179+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 230) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7253 bytes)
[2025-11-09T15:16:52.188+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on 172.19.0.9:35743 (size: 123.7 KiB, free: 434.3 MiB)
[2025-11-09T15:16:52.210+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 230) in 31 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ResultStage 90 (collectAsMap at UpsertPartitioner.java:282) finished in 0.047 s
[2025-11-09T15:16:52.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:52.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
[2025-11-09T15:16:52.211+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 43 finished: collectAsMap at UpsertPartitioner.java:282, took 0.048882 s
[2025-11-09T15:16:52.212+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.212+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.213+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO UpsertPartitioner: Total Buckets: 1
[2025-11-09T15:16:52.213+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251109151648443.deltacommit.requested
[2025-11-09T15:16:52.231+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251109151648443.deltacommit.inflight
[2025-11-09T15:16:52.246+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-11-09T15:16:52.246+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20251109151648443
[2025-11-09T15:16:52.272+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-11-09T15:16:52.273+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Registering RDD 192 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 23
[2025-11-09T15:16:52.273+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Got job 44 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-11-09T15:16:52.273+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Final stage: ResultStage 92 (collect at HoodieJavaRDD.java:177)
[2025-11-09T15:16:52.273+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)
[2025-11-09T15:16:52.273+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 91)
[2025-11-09T15:16:52.274+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ShuffleMapStage 91 (MapPartitionsRDD[192] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-11-09T15:16:52.283+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 357.3 KiB, free 433.6 MiB)
[2025-11-09T15:16:52.287+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 123.9 KiB, free 433.4 MiB)
[2025-11-09T15:16:52.287+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 863c17993851:44651 (size: 123.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.288+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.288+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 91 (MapPartitionsRDD[192] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.289+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.290+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 231) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7688 bytes)
[2025-11-09T15:16:52.297+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on 172.19.0.9:35743 (size: 123.9 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.311+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 231) in 22 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.311+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.312+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ShuffleMapStage 91 (mapToPair at HoodieJavaRDD.java:149) finished in 0.037 s
[2025-11-09T15:16:52.312+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: looking for newly runnable stages
[2025-11-09T15:16:52.312+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: running: Set()
[2025-11-09T15:16:52.313+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: waiting: Set(ResultStage 92)
[2025-11-09T15:16:52.313+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: failed: Set()
[2025-11-09T15:16:52.313+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[197] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-11-09T15:16:52.325+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 459.3 KiB, free 433.0 MiB)
[2025-11-09T15:16:52.328+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 162.5 KiB, free 432.8 MiB)
[2025-11-09T15:16:52.328+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 863c17993851:44651 (size: 162.5 KiB, free: 434.0 MiB)
[2025-11-09T15:16:52.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[197] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.329+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.331+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 232) (172.19.0.9, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-11-09T15:16:52.341+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on 172.19.0.9:35743 (size: 162.5 KiB, free: 434.0 MiB)
[2025-11-09T15:16:52.353+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 172.19.0.9:56384
[2025-11-09T15:16:52.364+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 863c17993851:44651 in memory (size: 123.7 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.366+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_60_piece0 on 172.19.0.9:35743 in memory (size: 123.7 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.371+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 863c17993851:44651 in memory (size: 5.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.372+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_58_piece0 on 172.19.0.9:35743 in memory (size: 5.5 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.377+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 863c17993851:44651 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.379+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_59_piece0 on 172.19.0.9:35743 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.394+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 863c17993851:44651 in memory (size: 123.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.395+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Removed broadcast_61_piece0 on 172.19.0.9:35743 in memory (size: 123.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.474+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added rdd_196_0 in memory on 172.19.0.9:35743 (size: 367.0 B, free: 434.2 MiB)
[2025-11-09T15:16:52.479+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 232) in 149 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ResultStage 92 (collect at HoodieJavaRDD.java:177) finished in 0.166 s
[2025-11-09T15:16:52.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:52.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
[2025-11-09T15:16:52.480+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 44 finished: collect at HoodieJavaRDD.java:177, took 0.208166 s
[2025-11-09T15:16:52.481+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
[2025-11-09T15:16:52.481+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseSparkCommitActionExecutor: Committing 20251109151648443, action Type deltacommit, operation Type UPSERT_PREPPED
[2025-11-09T15:16:52.502+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-11-09T15:16:52.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Got job 45 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-11-09T15:16:52.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Final stage: ResultStage 93 (collect at HoodieSparkEngineContext.java:150)
[2025-11-09T15:16:52.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:52.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:52.503+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[199] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-11-09T15:16:52.506+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 100.7 KiB, free 433.7 MiB)
[2025-11-09T15:16:52.509+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.7 MiB)
[2025-11-09T15:16:52.509+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 863c17993851:44651 (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[199] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.510+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.511+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 233) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-11-09T15:16:52.523+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on 172.19.0.9:35743 (size: 35.9 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.534+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 233) in 22 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.534+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.534+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ResultStage 93 (collect at HoodieSparkEngineContext.java:150) finished in 0.031 s
[2025-11-09T15:16:52.534+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:52.534+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
[2025-11-09T15:16:52.535+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 45 finished: collect at HoodieSparkEngineContext.java:150, took 0.032821 s
[2025-11-09T15:16:52.536+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Marking instant complete [==>20251109151648443__deltacommit__INFLIGHT]
[2025-11-09T15:16:52.536+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251109151648443.deltacommit.inflight
[2025-11-09T15:16:52.557+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251109151648443.deltacommit
[2025-11-09T15:16:52.557+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Completed [==>20251109151648443__deltacommit__INFLIGHT]
[2025-11-09T15:16:52.557+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseSparkCommitActionExecutor: Committed 20251109151648443
[2025-11-09T15:16:52.577+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-11-09T15:16:52.578+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Got job 46 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-11-09T15:16:52.579+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Final stage: ResultStage 94 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-11-09T15:16:52.579+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:52.579+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:52.579+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[201] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-11-09T15:16:52.582+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 100.9 KiB, free 433.6 MiB)
[2025-11-09T15:16:52.584+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.5 MiB)
[2025-11-09T15:16:52.585+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 863c17993851:44651 (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.586+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.586+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[201] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-11-09T15:16:52.586+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks resource profile 0
[2025-11-09T15:16:52.587+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 234) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-11-09T15:16:52.596+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on 172.19.0.9:35743 (size: 36.0 KiB, free: 434.2 MiB)
[2025-11-09T15:16:52.631+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 234) in 43 ms on 172.19.0.9 (executor 0) (1/1)
[2025-11-09T15:16:52.631+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.631+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ResultStage 94 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.052 s
[2025-11-09T15:16:52.631+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:52.632+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished
[2025-11-09T15:16:52.632+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 46 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.054401 s
[2025-11-09T15:16:52.655+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/.temp/20251109151648443
[2025-11-09T15:16:52.656+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:52.661+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:52.666+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:52.667+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-11-09T15:16:52.671+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__deltacommit__COMPLETED__20251109151652547]}
[2025-11-09T15:16:52.677+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:52.682+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-11-09T15:16:52.682+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-11-09T15:16:52.687+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__deltacommit__COMPLETED__20251109151652547]}
[2025-11-09T15:16:52.688+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Marking instant complete [==>20251109151648443__commit__INFLIGHT]
[2025-11-09T15:16:52.688+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/20251109151648443.inflight
[2025-11-09T15:16:52.711+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/20251109151648443.commit
[2025-11-09T15:16:52.712+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Completed [==>20251109151648443__commit__INFLIGHT]
[2025-11-09T15:16:52.745+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-11-09T15:16:52.746+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Got job 47 (collectAsMap at HoodieSparkEngineContext.java:164) with 2 output partitions
[2025-11-09T15:16:52.747+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Final stage: ResultStage 95 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-11-09T15:16:52.747+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Parents of final stage: List()
[2025-11-09T15:16:52.747+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Missing parents: List()
[2025-11-09T15:16:52.747+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[203] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-11-09T15:16:52.753+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 100.9 KiB, free 433.4 MiB)
[2025-11-09T15:16:52.758+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.4 MiB)
[2025-11-09T15:16:52.759+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 863c17993851:44651 (size: 36.0 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.759+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1535
[2025-11-09T15:16:52.760+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 95 (MapPartitionsRDD[203] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1))
[2025-11-09T15:16:52.760+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Adding task set 95.0 with 2 tasks resource profile 0
[2025-11-09T15:16:52.761+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 235) (172.19.0.9, executor 0, partition 0, PROCESS_LOCAL, 7334 bytes)
[2025-11-09T15:16:52.762+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Starting task 1.0 in stage 95.0 (TID 236) (172.19.0.9, executor 0, partition 1, PROCESS_LOCAL, 7330 bytes)
[2025-11-09T15:16:52.774+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on 172.19.0.9:35743 (size: 36.0 KiB, free: 434.1 MiB)
[2025-11-09T15:16:52.806+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 1.0 in stage 95.0 (TID 236) in 45 ms on 172.19.0.9 (executor 0) (1/2)
[2025-11-09T15:16:52.826+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 235) in 65 ms on 172.19.0.9 (executor 0) (2/2)
[2025-11-09T15:16:52.826+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool
[2025-11-09T15:16:52.826+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: ResultStage 95 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.078 s
[2025-11-09T15:16:52.827+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-11-09T15:16:52.827+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
[2025-11-09T15:16:52.827+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO DAGScheduler: Job 47 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.081972 s
[2025-11-09T15:16:52.853+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=orders/.hoodie/.temp/20251109151648443
[2025-11-09T15:16:52.855+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseHoodieWriteClient: Committed 20251109151648443
[2025-11-09T15:16:52.856+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MapPartitionsRDD: Removing RDD 165 from persistence list
[2025-11-09T15:16:52.857+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManager: Removing RDD 165
[2025-11-09T15:16:52.858+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MapPartitionsRDD: Removing RDD 175 from persistence list
[2025-11-09T15:16:52.859+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManager: Removing RDD 175
[2025-11-09T15:16:52.859+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO UnionRDD: Removing RDD 186 from persistence list
[2025-11-09T15:16:52.860+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManager: Removing RDD 186
[2025-11-09T15:16:52.861+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO MapPartitionsRDD: Removing RDD 196 from persistence list
[2025-11-09T15:16:52.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BlockManager: Removing RDD 196
[2025-11-09T15:16:52.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseHoodieWriteClient: Async cleaner has been spawned. Waiting for it to finish
[2025-11-09T15:16:52.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AsyncCleanerService: Waiting for async clean service to finish
[2025-11-09T15:16:52.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseHoodieWriteClient: Async cleaner has finished
[2025-11-09T15:16:52.862+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.870+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:52.878+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.879+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.883+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__commit__COMPLETED__20251109151652699]}
[2025-11-09T15:16:52.884+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.890+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:52.896+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.897+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:52.901+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:52.906+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:52.910+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__deltacommit__COMPLETED__20251109151652547]}
[2025-11-09T15:16:52.910+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.910+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-11-09T15:16:52.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating remote first table view
[2025-11-09T15:16:52.911+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseHoodieWriteClient: Start to archive synchronously.
[2025-11-09T15:16:52.915+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__commit__COMPLETED__20251109151652699]}
[2025-11-09T15:16:52.916+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.922+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-11-09T15:16:52.928+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-11-09T15:16:52.928+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:52.933+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-11-09T15:16:52.938+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-11-09T15:16:52.942+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__deltacommit__COMPLETED__20251109151652547]}
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieTimelineArchiver: No Instants to archive
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating remote view for basePath s3a://huditest/silver/table_name=orders. Server=863c17993851:39921, Timeout=300
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=orders
[2025-11-09T15:16:52.943+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.947+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.953+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__commit__COMPLETED__20251109151652699]}
[2025-11-09T15:16:52.953+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO RemoteHoodieTableFileSystemView: Sending request : (http://863c17993851:39921/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fhuditest%2Fsilver%2Ftable_name%3Dorders&lastinstantts=20251109151648443&timelinehash=bed127e56715fcd1ce87c9ef0308351b7083dc7c1b1da2a5f6ddd444716b3c94)
[2025-11-09T15:16:52.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__commit__COMPLETED__20251109151652699]}
[2025-11-09T15:16:52.960+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.964+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.969+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__commit__COMPLETED__20251109151652699]}
[2025-11-09T15:16:52.974+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251109151648443__deltacommit__COMPLETED__20251109151652547]}
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Commit 20251109151648443 successful!
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Compaction Scheduled is Optional.empty
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Clustering Scheduled is Optional.empty
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Is Async Compaction Enabled ? false
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO HoodieSparkSqlWriter$: Closing write client
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO BaseHoodieClient: Stopping Timeline service !!
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO EmbeddedTimelineService: Closing Timeline server
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TimelineService: Closing Timeline Service
[2025-11-09T15:16:52.975+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO Javalin: Stopping Javalin ...
[2025-11-09T15:16:52.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO Javalin: Javalin has stopped
[2025-11-09T15:16:52.982+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO TimelineService: Closed Timeline Service
[2025-11-09T15:16:52.983+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO EmbeddedTimelineService: Closed Timeline server
[2025-11-09T15:16:52.983+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:52 INFO AsyncCleanerService: Shutting down async clean service...
[2025-11-09T15:16:53.461+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO SparkContext: Invoking stop() from shutdown hook
[2025-11-09T15:16:53.461+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-11-09T15:16:53.474+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO SparkUI: Stopped Spark web UI at http://863c17993851:4040
[2025-11-09T15:16:53.481+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-11-09T15:16:53.482+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-11-09T15:16:53.505+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-11-09T15:16:53.529+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO MemoryStore: MemoryStore cleared
[2025-11-09T15:16:53.529+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO BlockManager: BlockManager stopped
[2025-11-09T15:16:53.533+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-11-09T15:16:53.539+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-11-09T15:16:53.596+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO SparkContext: Successfully stopped SparkContext
[2025-11-09T15:16:53.597+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO ShutdownHookManager: Shutdown hook called
[2025-11-09T15:16:53.598+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-8d16732b-db55-4efd-98ef-d45607756a8b
[2025-11-09T15:16:53.606+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5a6d9e3-cfca-41d1-86fe-bc0566552d5b
[2025-11-09T15:16:53.613+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO ShutdownHookManager: Deleting directory /tmp/spark-f5a6d9e3-cfca-41d1-86fe-bc0566552d5b/pyspark-93745987-85e6-41ed-9034-1299413388b2
[2025-11-09T15:16:53.639+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-11-09T15:16:53.640+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-11-09T15:16:53.640+0000] {spark_submit.py:579} INFO - 25/11/09 15:16:53 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-11-09T15:16:53.721+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=dag_create_hudi_tables, task_id=python_job, execution_date=20251109T151615, start_date=20251109T151617, end_date=20251109T151653
[2025-11-09T15:16:53.765+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-11-09T15:16:53.786+0000] {taskinstance.py:2776} INFO - 1 downstream tasks scheduled from follow-on schedule check
